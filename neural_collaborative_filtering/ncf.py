# -*- coding: utf-8 -*-
"""Recbole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WSf_l0RkplIeQ47Cp4aSCiPqJ_vhTqis
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.utils import shuffle

import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import layers,Input, Model
from tensorflow.keras.layers import Dense, Concatenate, Multiply, Embedding,Dot, Dropout

u_data_link = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/u.data'
df = pd.read_csv(u_data_link,delimiter='\t',header=None)
col_titles = ['user_id', 'item_id' , 'rating' , 'timestamp']
df.columns = col_titles



num_users = len(df['user_id'].unique())
num_items = len(df['item_id'].unique())

print('number of different users :', num_users)
print('number of different items :', num_items)

#### so timestamp has been recorded as unix time stamp, I will convert it. 
pd.to_datetime(df['timestamp'], unit='s')

def time_conv(x):

  y = pd.to_datetime(x, unit='s')
  return y

df['timestamp'] = time_conv(df['timestamp'])

### I will only need time but not need the time, therefore, I will strip the time 
def data_splitter(x):
  return str(x).split()[0]

df['timestamp'] = df['timestamp'].apply(data_splitter)
df['timestamp'] =pd.to_datetime(df['timestamp'])

### So lets sort the users and items 
df = df.sort_values(by = ['user_id']).reset_index(drop = True)

unique_items = df['item_id'].unique()
unique_user = df['user_id'].unique()

user_2enc = {i:j for i,j in enumerate(df['user_id'].unique())}
enc_2user = {j:i for i,j in enumerate(df['user_id'].unique())}

item_2enc = {i:j for i,j in enumerate(df['item_id'].unique())}
enc_2item = {j:i for i,j in enumerate(df['item_id'].unique())}
df['user_id_enc'] = df['user_id'].map(enc_2user)
df['item_id_enc'] = df['item_id'].map(enc_2item)


encoded_user_list = df['user_id_enc'].unique()
encoded_item_list = df['item_id_enc'].unique()

unique_item_enc = df['item_id_enc'].unique()
unique_user_enc = df['user_id_enc'].unique()

df_enc = pd.DataFrame({'user_id_enc':  df['user_id'].map(enc_2user), 
                       'item_id_enc': df['item_id'].map(enc_2item),
                       'ratings':df['rating'].copy()})



def sequencer(df,col):
  var = df_2.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
  var = var.reset_index()

  return var

df_2 = df_enc[['user_id_enc','item_id_enc','ratings']]
df_2['ratings'] = 1
df_enc_seq =sequencer(df_2,'user_id_enc')
df_enc_seq.head(3)



def train_test_maker(df): ### df will be in sequential format
  last_items = []
  for i in range(len(df)):
    last_items.append(np.array(df['item_id_enc'][i][-1]))

  mid_items = []
  for i in range(len(df)):
    mid_items.append(df['item_id_enc'][i][:-1])

  return(last_items, mid_items)

last_items, mid_items = train_test_maker(df_enc_seq)

test_df_seq = pd.DataFrame({'user_id_enc':unique_user_enc,
                        'item_id_enc':np.array(last_items),
                        'ratings':np.ones(len(last_items),dtype = 'int32')})

train_df_seq = pd.DataFrame({'user_id_enc':unique_user_enc,
                        'item_id_enc':mid_items,
                        'ratings':np.ones(len(mid_items),dtype = 'int32')})

test_df = pd.DataFrame({'user_id_enc':df_enc_seq['user_id_enc'].copy(),
                        'item_id_enc':last_items, 
                        'ratings' : np.ones(len(last_items),dtype='int32') 
                         })
user_create = []
for i in range(len(mid_items)):
  # len(mid_items[i])
  user_create.append(np.full(len(mid_items[i]),i))

train_df = pd.DataFrame({'user_id_enc':np.concatenate(user_create),
                        'item_id_enc':np.concatenate(mid_items), 
                        'ratings' : np.ones(len(np.concatenate(user_create)),dtype='int32') 
                         })

num_neg = 4
neg3 = []
ui= []
for i in range(len(train_df_seq)):
  pos_items = train_df_seq['item_id_enc'][i]
  neg2 = []
  for k in range(num_neg):
  # for j in range(len(pos_items)):
    pos_item = pos_items[k]

    neg1=  [] 
    # for k in range(num_neg):
    for j in range(len(pos_items)):
      neg_item = np.random.randint(1, len(unique_item_enc ))

      while neg_item in pos_items:
        neg_item  = np.random.randint(1, len(unique_item_enc))

      neg1.append(neg_item)
      
    neg2.append(np.array(neg1))
    ui.append(i)
  neg3.append(np.array(neg2))

#### negative sample generator for the train dataset:
#### as input all train data must be sequentially sorted 
#### output will we sorted as items in an array according to corresponding user
#### i.e. user[0], neg_items[1,2,3,]

def neg_maker(num_negs, df):
  num_neg = num_negs
  neg3 = []
  ui = []

  for i in range(len(df)):
    pos_items = df['item_id_enc'][i]
    neg2 = []

    for k in range(num_neg):
      pos_item = pos_items[k]
      neg1 = []

      for j in range(len(pos_items)):
        neg_item = np.random.randint(1, len(unique_item_enc))

        while neg_item in pos_items:
          neg_item = np.random.randint(1, len(unique_item_enc))

        neg1.append(neg_item)

      neg2.append(np.array(neg1))
      ui.append(i)
    neg3.append(np.array(neg2))

  total_negs = []
  for i in range(len(train_df_seq)):
    total_negs.append(np.concatenate(neg3[i][:num_negs]))

  return total_negs

train_neg = neg_maker(4, train_df_seq)

train_ui = []
for i in range(len(train_neg)):
  train_ui.append(np.full(len(train_neg[i]),i))

train_neg_df = pd.DataFrame({'user_id_enc':np.concatenate(train_ui),
                        'item_id_enc':np.concatenate(train_neg), 
                        'ratings' : np.zeros(len(np.concatenate(train_ui)),dtype='int32') 
                         })

train_total = shuffle(pd.concat([train_df,train_neg_df])).reset_index(drop=True)

test_df_seq

num_neg = 40
neg2 = []
for i in range(len(test_df_seq)):
  pos_item = test_df_seq['item_id_enc'][i]
  neg1= []
  for j in range(num_neg):
    neg_candidate = np.random.randint(1, len(unique_item_enc))

    while neg_candidate in df_enc_seq['item_id_enc'][i]:
      neg_candidate = np.random.randint(1, len(unique_item_enc))
    
    neg1.append(neg_candidate)
  neg2.append(neg1)

test_negs = []
for i in range(len(test_df_seq)):
  test_negs.append(np.vstack(neg2[i][:num_neg]))

test_ui = []
for i in range(len(test_negs)):
  test_ui.append(np.full(len(test_negs[i]),i))

test_neg_df = pd.DataFrame({'user_id_enc':np.concatenate(test_ui),
                        'item_id_enc':np.squeeze(np.concatenate(np.array(test_negs))), 
                        'ratings' : np.zeros(len(np.concatenate(test_ui)),dtype='int32') 
                         })
test_total = shuffle(pd.concat([test_neg_df, test_df_seq])).reset_index(drop = True)

test_neg_df

# test_ui = []
# for i in range(len(test_neg)):
#   test_ui.append(np.full(len(test_neg[i]),i))

# train_neg_df = pd.DataFrame({'user_id_enc':np.concatenate(test_ui),
#                         'item_id_enc':np.concatenate(test_neg), 
#                         'ratings' : np.zeros(len(np.concatenate(test_ui)),dtype='int32') 
#                          })
# test_neg

emb_dim = 8

user_input = Input(shape = (1,), name = 'user_input')
item_input = Input(shape = (1,), name = 'item_input')

user_emb_mlp = Embedding(num_users+2, emb_dim, name = 'user_emb_mlp')(user_input)
item_emb_mlp = Embedding(num_items+2, emb_dim, name = 'item_emb_mlp')(item_input)

user_emb_gmf = Embedding(num_users+2, emb_dim, name = 'user_emb_gmf')(user_input)
item_emb_gmf = Embedding(num_items+2, emb_dim, name = 'item_emb_gmf')(item_input)

mult_layer = Multiply(name = 'element_wise_multiplication_gmf'
                    )([user_emb_gmf, item_emb_gmf])

concat_layer = Concatenate()([user_emb_mlp,item_emb_gmf])

dense_1 = Dense(64, activation = 'relu', name = 'dense_1_mlp')(concat_layer)
dense_1 = Dropout(0.5)(dense_1)
dense_2 = Dense(32, activation = 'relu', name = 'dense_2_mlp')(dense_1)
dense_2 = Dropout(0.5)(dense_2)
dense_3 = Dense(16, activation = 'relu', name = 'dense_3_mlp')(dense_2)
dense_3 = Dropout(0.5)(dense_3)
dense_4 = Dense(8, activation = 'relu')(dense_3)
# dense_5 = Dense(4, activation = 'relu')(dense_4)

neuMF_layer = Concatenate(axis=-1, name = 'NeuMF_layer')([mult_layer,dense_4])

final = Dense(1, activation = 'sigmoid', name = 'final_layer')(neuMF_layer)

mlp_model = Model(inputs = [user_input, item_input], outputs = [final])

mlp_model.compile(
    # loss = tf.keras.losses.BinaryCrossentropy(),
    loss = 'BinaryCrossentropy',
    optimizer = 'Adam',
    metrics = ['accuracy']
    )

mlp_hist = mlp_model.fit([train_total['user_id_enc'], train_total['item_id_enc']],
              train_total['ratings'],
              validation_split = 0.2,
              epochs = 20,
              verbose = 0
              )

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.suptitle('A tale of 2 subplots')

ax1.plot(mlp_hist.history['loss'],label = 'train_loss')
ax1.plot(mlp_hist.history['val_loss'], label = 'val_loss')
# ax1.title('Loss vs Epochs')
# ax1.xlabel('# Epocs')
# ax1.ylabel('loss')
# ax1.legend()

ax2.plot(mlp_hist.history['accuracy'],label='train_acc')
ax2.plot(mlp_hist.history['val_accuracy'],label='val_acc')
# ax2.title('Accuracy vs Epochs')
# ax2.xlabel('# Epocs')
# ax2.ylabel('loss')
# ax2.legend()

plt.show()

predict = (mlp_model.predict([test_total['user_id_enc'], test_total['item_id_enc']]))

print(confusion_matrix(test_total['ratings'].values, np.squeeze(np.round(predict))))

print(classification_report(test_total['ratings'].values, np.squeeze(np.round(predict))))

# neg_items = []
# num_neg = 4
# ui = []
# for i in range(len(unique_user_enc)):
#   pos_items = np.array(df_enc[df_enc['user_id_enc']== i]['item_id_enc'])
  
#   neg_items1 = []
#   for j in range(len(pos_items)):
#     neg_items2 = []
#     for k in range(num_neg):
#       neg_selection = np.random.randint(1,len(unique_item_enc))

#       while neg_selection in pos_items:
#         neg_selection = np.random.randint(1,len(unique_item_enc))

#       neg_items2.append(neg_selection)
#       ui.append(i)
#     neg_items1.append(neg_items2)
#   neg_items.append(np.array(neg_items1))

# u = 4
# t = []
# for i in range(num_users):
#   x = np.array(neg_frame[neg_frame['user_id_enc'] == u]['item_id_enc'])
#   y = np.array(df_enc[df_enc['user_id_enc']== u]['item_id_enc'])
#   t.append(np.unique(np.in1d(x,y)))
# np.unique(t)

import heapq
def get_hits(k_ranked, holdout):

        for item in k_ranked:
            if item == holdout:
                return 1
        return 0

def eval_rating(idx, test_ratings, test_negatives, K, model):

    items = test_negatives[idx]      # negative items [neg_item_id, ... ] (1,100)
    user_idx = test_ratings[idx][0]  # [user_id, item_id][0]
    holdout = test_ratings[idx][1]   # [user_id, item_id][1]
    items.append(holdout)            # holdout 추가 [neg_item_id, ..., holdout] (1,101)

        # prediction
    predict_user = np.full(len(items), user_idx, dtype='int32').reshape(-1, 1)  # [[user_id], ...], (101, 1)
    np_items = np.array(items).reshape(-1, 1)                                   # [[item_id], ... ], (101, 1)

    predictions = model.predict([predict_user, np_items])
    predictions = predictions.flatten().tolist()
    item_to_pre_score = {item:pre for item, pre in zip(items, predictions)}

        # 점수가 높은 상위 k개 아이템 리스트
    k_ranked = heapq.nlargest(K, item_to_pre_score, key=item_to_pre_score.get)

        # holdout이 상위 K 순위에 포함 되는지 체크
        # {1:포함, 0:포함x}
    hits = get_hits(k_ranked, holdout)

    return hits

def evaluate_top_k(df_neg, df_test, model, K=10):

    hits = []
    test_u = df_test['user_id_enc'].values.tolist()
    test_i = df_test['item_id_enc'].values.tolist()

    test_ratings = list(zip(test_u, test_i))
    # df_neg = df_neg.drop(df_neg.columns[0], axis=1)
    test_negatives = df_neg.values.tolist()  # [[(user_id, item_id=holdout)], neg_item,... ] (1,100)

        # user 샘플링
    # sample_idx_lst = np.random.choice(len(test_ratings), int(len(test_ratings) * 0.03))
    sample_idx_lst = np.random.choice(900, int(len(test_ratings) * 0.3))

    for user_idx in sample_idx_lst:  # 전체 사용: range(len(test_ratings))

        hitrate = eval_rating(user_idx, test_ratings, test_negatives, K, model)
        hits.append(hitrate)  # ex. [1,0,1,1,0,...] (1, df_test.shape[0])

    return hits

def sequencer2(df,col):
  var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
  var = var.reset_index()

  return var

x = sequencer2(test_neg_df,'user_id_enc')
x

a = list([1,2,3])
b = list([2,3,4])

pd.DataFrame([a,b])

y = (x['item_id_enc'].to_numpy())

negs_resorted = []
negs_resorted2 = []
for i in range(len(x)) :
  var1 = np.array(x['item_id_enc'][i]).reshape(1, len(np.array(x['item_id_enc'][i])))
  # print(var1)
  negs_resorted.append(np.pad(var1, (0,num_neg- len(var1)), 'constant'))
  negs_resorted2.append((var1))
d = []

for i in range(len(negs_resorted2)):
  d.append(tf.keras.preprocessing.sequence.pad_sequences(
    negs_resorted2[i],
    maxlen=num_neg,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
  )
  )
f = np.squeeze(np.array(d),axis = 1)

df_neg = pd.DataFrame(f).drop(columns = [0])

df_neg

hit_list = evaluate_top_k(df_neg, test_neg_df, mlp_model, K=10)
np.mean(hit_list)

