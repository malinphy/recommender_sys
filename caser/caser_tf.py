# -*- coding: utf-8 -*-
"""j_junk1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Bv-lxuk30xiTPC2V2MJFF92sIJmlP4o
"""

import numpy as np 
import pandas as pd 
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import Dense, Lambda,Embedding, Conv1D,Conv2D
from tensorflow.keras.layers import MaxPool1D,Dropout, Lambda, Concatenate, Flatten
from tensorflow.keras.layers import Multiply,Add
import matplotlib.pyplot as plt

class dataprep:
  def __init__(self,num_targets, num_negs,seq_len):
    self.num_targets = num_targets
    self.num_negs = num_negs
    self.seq_len = seq_len

  def sequence_df(self):
    url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/ratings.csv'
    df = pd.read_csv(url)
    df.rename(columns = {'userId':'user_id', 'movieId':'item_id'}, inplace = True)
  # df
    df['timestamp']= pd.to_datetime(df['timestamp'], unit = 's')
    df.head(10)

    def id_converter(df,x):
      unique_params = df[x].unique()
      num_unique = len(unique_params)
      param_2enc = { i:j for i,j in enumerate(unique_params)}
      enc_2param = { j:i for i,j in enumerate(unique_params)}


      return num_unique , param_2enc, enc_2param

    num_users, user_2enc ,enc_2user = id_converter(df,'user_id')
    num_items, item_2enc ,enc_2item = id_converter(df,'item_id')

    df['user_id'] = df['user_id'].map(enc_2user)
    df['item_id'] = df['item_id'].map(enc_2item)

    def time_sorter(df):
    ### users and releated items were sorted according to time as function of increasing time 
      return(df.set_index(['user_id','timestamp']).sort_index().reset_index())

    time_sorted_df = time_sorter(df)
    time_sorted_df = time_sorted_df.drop(columns= ['timestamp','rating'])   
    time_sorted_df.head(2)

    def users_grouped(df,x):
      return df.groupby([x]).aggregate(lambda tdf: tdf.unique().tolist()).reset_index()
    # df = df.reset_index()
    
    grouped_df = users_grouped(time_sorted_df,'user_id')
    grouped_df.head(3)

    # def sequence_creator(df, self.seq_len, self.num_targets):
    def sequence_creator(df):
      users = []
      seqs = []
      targets = []

      for i in range(len(grouped_df)):

        for j in range(0,len(grouped_df['item_id'][i])-self.seq_len,self.num_targets):
          users.append(i)
          x = grouped_df['item_id'][i][j:j+self.seq_len+self.num_targets]
          x1 = grouped_df['item_id'][i][j:j+self.seq_len]
          seqs.append(list(x[0:self.seq_len]))
          targets.append(np.array(x[-self.num_targets:]))

      return (pd.DataFrame({'users':users,'sequences':seqs, 'targets':targets}))

    L = 5
    d = 2
    sequenced_df = sequence_creator(grouped_df)
    sequenced_df.head(2)

    d = 2

    tot2 = []
    for j in range(self.num_negs):
      tot = []
      for i in range(len(sequenced_df)):
        sampled_neg = np.random.randint(num_items)
        x  = sequenced_df['users'][i]
        grouped_df['item_id'][x]
        while ((sampled_neg in sequenced_df['sequences'][i]) and  (sampled_neg in sequenced_df['sequences'][i] in grouped_df['item_id'][x] ) ):
          sampled_neg = np.random.randint(num_items)
        tot.append(sampled_neg)
  
      tot2.append(np.array(tot))

    negs = np.array(tot2).T

    sequenced_df['negs'] = list(negs)


    return sequenced_df, num_users, num_items

x = dataprep(1,1,5)
df,num_users, num_items = x.sequence_df()
df.head(1)

def expander1(x):
    return (x[:,:,:,tf.newaxis])
    
def seqer2(x):
    return (tf.squeeze(x, axis=2))

def seqer1(x):
    return (tf.squeeze(x, axis=1))

TARGET_NUM = 1
batch_size = 4096
L=5
d=16
d_prime=4 
drop_ratio=0.05
num_factors= 10
dims = num_factors

doublet = np.squeeze(np.dstack([df['targets'].astype('int'), df['negs'].astype('int')]))
k = []
sayac = 1
for i in df['sequences']:
  t =[]
  for j in i:
    t.append(int(j))
  k.append(t)

y_dummy = tf.ones( [len(df['users']),1])

user_input = tf.keras.Input(shape = (1,), name = 'user_input')  ### kullanildi
seq_input = tf.keras.Input(shape = (L,), name = 'seq_input')  ### kullanildi
doublet_input = tf.keras.Input(shape = (2,), name = 'doublet_input')

seq_embedding = Embedding(num_items,num_factors, name = 'sequence_embedding')(seq_input)
seq_embedding = tf.expand_dims(seq_embedding, 3, name = 'sequence_embedding_expander')
doublet_embedding = Embedding(num_items, num_factors*2,
                              name = 'doublet_embedding')(doublet_input)## buyuk W

doublet_embedding_b = Embedding(num_items, 1,
                               name = 'doublet_embedding_1')(doublet_input)## kucuk b  

user_embedding = Embedding(num_users, num_factors,
                               name = 'user_embedding')(user_input)

h = [i + 1 for i in range(L)]
fc1_dim_v = d_prime * num_factors
out_v = Conv2D(d_prime,(L,1), name = 'convo_v')(seq_embedding)
out_v = Flatten(name = 'flatten_convo_v')(out_v)

out_hs = []
for i in (h):
  seq_mod = tf.keras.Sequential([
           Conv2D(d, (i, num_factors)),
           Lambda(seqer2),
           MaxPool1D(L - i + 1),
           Lambda(seqer1)                      
          ])
  out_hs.append(seq_mod(seq_embedding))

out_h = tf.concat(out_hs, axis=1, name ='sequential_out') # galiba axis 2 olmali
out = tf.concat([out_v, out_h], axis = 1, name ='convo_concat')
dropout_layer = Dropout(0.05, name = 'dropout_layer')(out)
z = Dense(10, name = 'dense_layer')(dropout_layer)
x = tf.concat([z, Flatten()(user_embedding)], axis=1, name = 'x')

print('X',x.shape)
print('doublet_embedding',doublet_embedding.shape)
print('doublet_embedding_b',doublet_embedding_b.shape)



@tf.function
def identity_loss(y_target, y_pred):
# def identity_loss(y_target, y_pred):  
  pos, neg = tf.split(y_pred,2,1)
  positive_loss = -1*tf.math.reduce_mean(tf.math.log(tf.sigmoid(pos)))
  negative_loss = -1*tf.math.reduce_mean(tf.math.log(1- tf.sigmoid(neg)))
  total_loss = tf.math.add(positive_loss, negative_loss)
  # return tf.math.reduce_mean(y_pred)
  return total_loss
caser_model =tf.keras.Model(inputs= [
                                      user_input,
                                      seq_input,
                                      doublet_input
                                      ],
                            # outputs = res2,
                            outputs = x
                            )

caser_model.compile(loss = identity_loss,optimizer = 'Adam')

caser_hist = caser_model.fit(
                            [
                              tf.constant(df['users']),
                              tf.constant(k),
                              tf.constant( np.squeeze( np.dstack( [ df[ 'targets'].astype('int'), df['negs'].astype('int') ] ) ) ) ],
                y_dummy,
                epochs = 1,  
                batch_size = 1 ### BATCH SIZE DEGISINCE PROGRAM HATA VERIYOR, BUNU DUZELT
                )

topk = 3
score = tf.matmul(x, tf.transpose(doublet_embedding, [0,2,1]))
pred= tf.math.reduce_sum(score, axis=1) +doublet_embedding_b
score_shape = tf.shape(score)
sb = tf.tile(tf.expand_dims(tf.squeeze(doublet_embedding_b),[0]),[score_shape[0],1])
score =  tf.sigmoid(score + sb)
top_k = tf.nn.top_k(score, k=topk)

print(pred.shape)
print(doublet_embedding.shape)
print(score_shape)

tf.squeeze(x, axis= 0)

deneme = tf.matmul(tf.squeeze(x, axis = 0), tf.squeeze(tf.transpose(doublet_embedding, [0,2,1])))
tf.shape(deneme)

d1 = tf.reshape(tf.squeeze(x, axis = 0),[1,20])
print(tf.shape(d1))

d2 = tf.squeeze(tf.transpose(doublet_embedding, [0,2,1]),axis = 0)
print(tf.shape(d2))

d3 = tf.matmul(d1, (d2))
d3_shape = tf.shape(d3)
tf.tile(tf.expand_dims(tf.squeeze(doublet_embedding_b),[0]),[score_shape[0],1])

