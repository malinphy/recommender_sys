# -*- coding: utf-8 -*-
"""j_junk1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Bv-lxuk30xiTPC2V2MJFF92sIJmlP4o
"""

import numpy as np 
import pandas as pd 
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import Dense, Lambda,Embedding, Conv1D,Conv2D
from tensorflow.keras.layers import MaxPool1D,Dropout, Lambda, Concatenate, Flatten
from tensorflow.keras.layers import Multiply,Add
import matplotlib.pyplot as plt

class dataprep:
  def __init__(self,num_targets, num_negs,seq_len):
    self.num_targets = num_targets
    self.num_negs = num_negs
    self.seq_len = seq_len

  def sequence_df(self):
    url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/ratings.csv'
    df = pd.read_csv(url)
    df.rename(columns = {'userId':'user_id', 'movieId':'item_id'}, inplace = True)
  # df
    df['timestamp']= pd.to_datetime(df['timestamp'], unit = 's')
    df.head(10)

    def id_converter(df,x):
      unique_params = df[x].unique()
      num_unique = len(unique_params)
      param_2enc = { i:j for i,j in enumerate(unique_params)}
      enc_2param = { j:i for i,j in enumerate(unique_params)}


      return num_unique , param_2enc, enc_2param

    num_users, user_2enc ,enc_2user = id_converter(df,'user_id')
    num_items, item_2enc ,enc_2item = id_converter(df,'item_id')

    df['user_id'] = df['user_id'].map(enc_2user)
    df['item_id'] = df['item_id'].map(enc_2item)

    def time_sorter(df):
    ### users and releated items were sorted according to time as function of increasing time 
      return(df.set_index(['user_id','timestamp']).sort_index().reset_index())

    time_sorted_df = time_sorter(df)
    time_sorted_df = time_sorted_df.drop(columns= ['timestamp','rating'])   
    time_sorted_df.head(2)

    def users_grouped(df,x):
      return df.groupby([x]).aggregate(lambda tdf: tdf.unique().tolist()).reset_index()
    # df = df.reset_index()
    
    grouped_df = users_grouped(time_sorted_df,'user_id')
    grouped_df.head(3)

    # def sequence_creator(df, self.seq_len, self.num_targets):
    def sequence_creator(df):
      users = []
      seqs = []
      targets = []

      for i in range(len(grouped_df)):

        for j in range(0,len(grouped_df['item_id'][i])-self.seq_len,self.num_targets):
          users.append(i)
          x = grouped_df['item_id'][i][j:j+self.seq_len+self.num_targets]
          x1 = grouped_df['item_id'][i][j:j+self.seq_len]
          seqs.append(list(x[0:self.seq_len]))
          targets.append(np.array(x[-self.num_targets:]))

      return (pd.DataFrame({'users':users,'sequences':seqs, 'targets':targets}))

    L = 5
    d = 2
    sequenced_df = sequence_creator(grouped_df)
    sequenced_df.head(2)

    d = 2

    tot2 = []
    for j in range(self.num_negs):
      tot = []
      for i in range(len(sequenced_df)):
        sampled_neg = np.random.randint(num_items)
        x  = sequenced_df['users'][i]
        grouped_df['item_id'][x]
        while ((sampled_neg in sequenced_df['sequences'][i]) and  (sampled_neg in sequenced_df['sequences'][i] in grouped_df['item_id'][x] ) ):
          sampled_neg = np.random.randint(num_items)
        tot.append(sampled_neg)
  
      tot2.append(np.array(tot))

    negs = np.array(tot2).T

    sequenced_df['negs'] = list(negs)


    return sequenced_df, num_users, num_items

x = dataprep(1,1,5)
df,num_users, num_items = x.sequence_df()
df.head(1)

def expander1(x):
    return (x[:,:,:,tf.newaxis])
    
def seqer2(x):
    return (tf.squeeze(x, axis=2))

def seqer1(x):
    return (tf.squeeze(x, axis=1))

TARGET_NUM = 1
batch_size = 4096
L=5
d=16
d_prime=4 
drop_ratio=0.05
num_factors= 10
dims = num_factors

doublet = np.squeeze(np.dstack([df['targets'].astype('int'), df['negs'].astype('int')]))
k = []
sayac = 1
for i in df['sequences']:
  t =[]
  for j in i:
    t.append(int(j))
  k.append(t)

y_dummy = tf.ones( [len(df['users']),1])

user_input = tf.keras.Input(shape = (1,), name = 'user_input')  ### kullanildi
seq_input = tf.keras.Input(shape = (L,), name = 'seq_input')  ### kullanildi
doublet_input = tf.keras.Input(shape = (2,), name = 'doublet_input')

seq_embedding = Embedding(num_items,num_factors, name = 'sequence_embedding')(seq_input)
seq_embedding = tf.expand_dims(seq_embedding, 3, name = 'sequence_embedding_expander')
doublet_embedding = Embedding(num_items, num_factors*2,
                              name = 'doublet_embedding')(doublet_input)## buyuk W

doublet_embedding_b = Embedding(num_items, 1,
                               name = 'doublet_embedding_1')(doublet_input)## kucuk b  

user_embedding = Embedding(num_users, num_factors,
                               name = 'user_embedding')(user_input)

h = [i + 1 for i in range(L)]
fc1_dim_v = d_prime * num_factors
out_v = Conv2D(d_prime,(L,1), name = 'convo_v')(seq_embedding)
out_v = Flatten(name = 'flatten_convo_v')(out_v)

out_hs = []
for i in (h):
  seq_mod = tf.keras.Sequential([
           Conv2D(d, (i, num_factors)),
           Lambda(seqer2),
           MaxPool1D(L - i + 1),
           Lambda(seqer1)                      
          ])
  out_hs.append(seq_mod(seq_embedding))

out_h = tf.concat(out_hs, axis=1, name ='sequential_out') # galiba axis 2 olmali
out = tf.concat([out_v, out_h], axis = 1, name ='convo_concat')
dropout_layer = Dropout(0.05, name = 'dropout_layer')(out)
z = Dense(10, name = 'dense_layer')(dropout_layer)
flat_users = Flatten(name = 'flatten')(user_embedding)
x = tf.concat([z, flat_users], axis=1, name = 'x')
# x = tf.concat([z, Flatten()(user_embedding)], axis=1, name = 'x')
res = tf.matmul(doublet_embedding,tf.expand_dims(x,axis=-1))
res = res+doublet_embedding_b
res = tf.squeeze(res,axis =2)

print('X',x.shape)
print('doublet_embedding',doublet_embedding.shape)
print('doublet_embedding_b',doublet_embedding_b.shape)



@tf.function
def identity_loss(y_target, y_pred):
# def identity_loss(y_target, y_pred):  
  pos, neg = tf.split(y_pred,2,1)
  positive_loss = -1*tf.math.reduce_mean(tf.math.log(tf.sigmoid(pos)))
  negative_loss = -1*tf.math.reduce_mean(tf.math.log(1- tf.sigmoid(neg)))
  total_loss = tf.math.add(positive_loss, negative_loss)
  # return tf.math.reduce_mean(y_pred)
  return total_loss
caser_model =tf.keras.Model(inputs= [
                                      user_input,
                                      seq_input,
                                      doublet_input
                                      ],
                            outputs = res,
#                             outputs = x
                            )

caser_model.compile(loss = identity_loss,optimizer = 'Adam')

caser_hist = caser_model.fit(
                            [
                              tf.constant(df['users']),
                              tf.constant(k),
                              tf.constant( np.squeeze( np.dstack( [ df[ 'targets'].astype('int'), df['negs'].astype('int') ] ) ) ) ],
                y_dummy,
                epochs = 1,  
                batch_size = 1 
                )


# -*- coding: utf-8 -*-
"""CASER_WITH_INTERACTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpQOXKGWxkwR23AMCU_w3lrB835CUhei
"""

import numpy as np 
import pandas as pd 
from google.colab import drive
drive.mount('/content/gdrive')
import os 
import scipy.sparse as sp

train_root = 'gdrive/MyDrive/Colab Notebooks/caser_pytorch-master/datasets/ml1m/test/train.txt'
test_root = 'gdrive/MyDrive/Colab Notebooks/caser_pytorch-master/datasets/ml1m/test/test.txt'

import numpy as np 
import pandas as pd 
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.layers import Dense, Lambda,Embedding, Conv1D,Conv2D
from tensorflow.keras.layers import MaxPool1D,Dropout, Lambda, Concatenate, Flatten
from tensorflow.keras.layers import Multiply,Add
import matplotlib.pyplot as plt

class Interactions(object):
    """
    Interactions object. Contains (at a minimum) pair of user-item
    interactions. This is designed only for implicit feedback scenarios.
    Parameters
    ----------
    file_path: file contains (user,item,rating) triplets
    user_map: dict of user mapping
    item_map: dict of item mapping
    """

    def __init__(self, file_path,
                 user_map=None,
                 item_map=None):

        if not user_map and not item_map:
            user_map = dict()
            item_map = dict()

            num_user = 0
            num_item = 0
        else:
            num_user = len(user_map)
            num_item = len(item_map)

        user_ids = list()
        item_ids = list()
        # read users and items from file
        with open(file_path, 'r') as fin:
            for line in fin:
                u, i, _ = line.strip().split()
                user_ids.append(u)
                item_ids.append(i)

        # update user and item mapping
        for u in user_ids:
            if u not in user_map:
                user_map[u] = num_user
                num_user += 1
        for i in item_ids:
            if i not in item_map:
                item_map[i] = num_item
                num_item += 1

        user_ids = np.array([user_map[u] for u in user_ids])
        item_ids = np.array([item_map[i] for i in item_ids])

        self.num_users = num_user
        self.num_items = num_item

        self.user_ids = user_ids
        self.item_ids = item_ids

        self.user_map = user_map
        self.item_map = item_map

        self.sequences = None
        self.test_sequences = None

    def __len__(self):

        return len(self.user_ids)

    def tocoo(self):
        """
        Transform to a scipy.sparse COO matrix.
        """

        row = self.user_ids
        col = self.item_ids
        data = np.ones(len(self))

        return sp.coo_matrix((data, (row, col)),
                             shape=(self.num_users, self.num_items))

    def tocsr(self):
        """
        Transform to a scipy.sparse CSR matrix.
        """

        return self.tocoo().tocsr()

    def to_sequence(self, sequence_length=5, target_length=1):
        """
        Transform to sequence form.
        Valid subsequences of users' interactions are returned. For
        example, if a user interacted with items [1, 2, 3, 4, 5, 6, 7, 8, 9], the
        returned interactions matrix at sequence length 5 and target length 3
        will be be given by:
        sequences:
           [[1, 2, 3, 4, 5],
            [2, 3, 4, 5, 6],
            [3, 4, 5, 6, 7]]
        targets:
           [[6, 7],
            [7, 8],
            [8, 9]]
        sequence for test (the last 'sequence_length' items of each user's sequence):
        [[5, 6, 7, 8, 9]]
        Parameters
        ----------
        sequence_length: int
            Sequence length. Subsequences shorter than this
            will be left-padded with zeros.
        target_length: int
            Sequence target length.
        """

        # change the item index start from 1 as 0 is used for padding in sequences
        for k, v in self.item_map.items():
            self.item_map[k] = v + 1
        self.item_ids = self.item_ids + 1
        self.num_items += 1

        max_sequence_length = sequence_length + target_length

        # Sort first by user id
        sort_indices = np.lexsort((self.user_ids,))

        user_ids = self.user_ids[sort_indices]
        item_ids = self.item_ids[sort_indices]

        user_ids, indices, counts = np.unique(user_ids,
                                              return_index=True,
                                              return_counts=True)

        num_subsequences = sum([c - max_sequence_length + 1 if c >= max_sequence_length else 1 for c in counts])

        sequences = np.zeros((num_subsequences, sequence_length),
                             dtype=np.int64)
        sequences_targets = np.zeros((num_subsequences, target_length),
                                     dtype=np.int64)
        sequence_users = np.empty(num_subsequences,
                                  dtype=np.int64)

        test_sequences = np.zeros((self.num_users, sequence_length),
                                  dtype=np.int64)
        test_users = np.empty(self.num_users,
                              dtype=np.int64)

        _uid = None
        for i, (uid,
                item_seq) in enumerate(_generate_sequences(user_ids,
                                                           item_ids,
                                                           indices,
                                                           max_sequence_length)):
            if uid != _uid:
                test_sequences[uid][:] = item_seq[-sequence_length:]
                test_users[uid] = uid
                _uid = uid
            sequences_targets[i][:] = item_seq[-target_length:]
            sequences[i][:] = item_seq[:sequence_length]
            sequence_users[i] = uid

        self.sequences = SequenceInteractions(sequence_users, sequences, sequences_targets)
        self.test_sequences = SequenceInteractions(test_users, test_sequences)

        return self.sequences, self.test_sequences
        
class SequenceInteractions(object):
    """
    Interactions encoded as a sequence matrix.
    Parameters
    ----------
    user_ids: np.array
        sequence users
    sequences: np.array
        The interactions sequence matrix, as produced by
        :func:`~Interactions.to_sequence`
    targets: np.array
        sequence targets
    """

    def __init__(self,
                 user_ids,
                 sequences,
                 targets=None):
        self.user_ids = user_ids
        self.sequences = sequences
        self.targets = targets

        self.L = sequences.shape[1]
        self.T = None
        if np.any(targets):
            self.T = targets.shape[1]


def _sliding_window(tensor, window_size, step_size=1):
    if len(tensor) - window_size >= 0:
        for i in range(len(tensor), 0, -step_size):
            if i - window_size >= 0:
                yield tensor[i - window_size:i]
            else:
                break
    else:
        num_paddings = window_size - len(tensor)
        # Pad sequence with 0s if it is shorter than windows size.
        yield np.pad(tensor, (num_paddings, 0), 'constant')


def _generate_sequences(user_ids, item_ids,
                        indices,
                        max_sequence_length):
    for i in range(len(indices)):

        start_idx = indices[i]

        if i >= len(indices) - 1:
            stop_idx = None
        else:
            stop_idx = indices[i + 1]

        for seq in _sliding_window(item_ids[start_idx:stop_idx],
                                   max_sequence_length):
            yield (user_ids[i], seq)

L = 5
T = 1
train = Interactions(train_root)
train.to_sequence(L, T)

test = Interactions(test_root,
                        user_map=train.user_map,
                        item_map=train.item_map)
test.to_sequence(L,T)

train_sequences = (train.sequences.sequences)
train_targets = np.array(train.sequences.targets)
train_users= np.array(train.sequences.user_ids.reshape(-1, 1))

test_sequences = test.sequences.sequences
test_targets = test.sequences.targets
test_users = test.sequences.user_ids.reshape(-1, 1)

print('print train num user',train.num_users)
print('print train num items',train.num_items)
print('print test num user',test.num_users)
print('print test num items',test.num_items)

def _generate_negative_samples(users, interactions, n):
    """
        Sample negative from a candidate set of each user. The
        candidate set of each user is defined by:
        {All Items} \ {Items Rated by User}
        Parameters
        ----------
        users: array of np.int64
            sequence users
        interactions: :class:`spotlight.interactions.Interactions`
            training instances, used for generate candidates
        n: int
            total number of negatives to sample for each sequence
    """
    _candidate = dict()
    users_ = users.squeeze()
    negative_samples = np.zeros((users_.shape[0], n), np.int64)
    if not _candidate:
        all_items = np.arange(interactions.num_items - 1) + 1  # 0 for padding
        train = interactions.tocsr()
        for user, row in enumerate(train):
            _candidate[user] = list(set(all_items) - set(row.indices))

    for i, u in enumerate(users_):
        for j in range(n):
            x = _candidate[u]
            negative_samples[i, j] = x[
                np.random.randint(len(x))]
    print('NEGATIVES')
    return negative_samples

train_negatives = _generate_negative_samples(train_users, train, n=1)
test_negatives = _generate_negative_samples(test_users, test, n=1)

train_df = pd.DataFrame({'train_users':train_users.ravel(),
                         'train_sequences':list(train_sequences),
                         'train_targets':train_targets.ravel(),
                         'train_negatives':train_negatives.ravel()
                         })

test_df = pd.DataFrame({'test_users':test_users.ravel(),
                         'test_sequences':list(test_sequences),
                         'test_targets':test_targets.ravel(),
                         'test_negatives':test_negatives.ravel()
                         })

train_df

def expander1(x):
    return (x[:,:,:,tf.newaxis])
    
def seqer2(x):
    return (tf.squeeze(x, axis=2))

def seqer1(x):
    return (tf.squeeze(x, axis=1))

TARGET_NUM = 1
batch_size = 4096
L=5
d=16
d_prime=4 
drop_ratio=0.05
num_factors = 5
# num_factors = 10
dims = num_factors

# doublet = np.squeeze(np.dstack([train_df['train_targets'].astype('int'),train_df['train_negatives'].astype('int')]))
# k = []
# sayac = 1
# for i in train_df['train_sequences']:
#   t =[]
#   for j in i:
#     t.append(int(j))
#   k.append(t)

y_dummy = tf.ones( [len(train_df['train_users']),1])

train_sequences = [np.array(i)  for i in (train_df['train_sequences'])]
test_sequences = [np.array(i)  for i in (test_df['test_sequences'])]

num_items_train = train.num_items
num_users = train.num_users

test.num_items
num_items_test = test.num_items

user_input = tf.keras.Input(shape = (1,), name = 'user_input')  ### kullanildi
seq_input = tf.keras.Input(shape = (L,), name = 'seq_input')  ### kullanildi

seq_embedding = Embedding(num_items_test,num_factors,
                          name = 'sequence_embedding')

seq_embedding_layer = seq_embedding(seq_input)

seq_embedding_l = tf.expand_dims(seq_embedding_layer, 3,
                               name = 'sequence_embedding_expander')

# doublet_input = tf.keras.Input(shape = (2,), name = 'doublet_input')
# doublet_embedding = Embedding(num_items, num_factors*2,
#                               name = 'doublet_embedding')(doublet_input)## buyuk W

user_embedding = Embedding(num_users, num_factors,
                               name = 'user_embedding')

user_embedding_layer = user_embedding(user_input)                               

h = [i + 1 for i in range(L)]
fc1_dim_v = d_prime * num_factors
out_v = Conv2D(d_prime,(L,1), name = 'convo_v')(seq_embedding_l)
out_v = Flatten(name = 'flatten_convo_v')(out_v)
out_hs = []
for i in (h):
  seq_mod = tf.keras.Sequential([
           Conv2D(d, (i, num_factors)),
           Lambda(seqer2),
           MaxPool1D(L - i + 1),
           Lambda(seqer1)                      
          ])
  out_hs.append(seq_mod(seq_embedding_l))
out_h = tf.concat(out_hs, axis=1, name ='sequential_out') # galiba axis 2 olmali
out = tf.concat([out_v, out_h], axis = 1, name ='convo_concat')
dropout_layer = Dropout(0.05, name = 'dropout_layer')(out)
z = Dense(dims, name = 'dense_layer', activation = 'relu')(dropout_layer)
flat_users = Flatten(name = 'flatten')(user_embedding_layer)
x = tf.concat([z, flat_users], axis=1, name = 'concat_layer_x')
# x = Dense(5)(x)
#### step1

doublet_input = tf.keras.Input(shape = (None,),dtype=tf.int64, name = 'doublet_input')
doublet_embedding = Embedding(num_items_test, num_factors*2,
                              name = 'doublet_embedding')## buyuk W
doublet_embedding_b = Embedding(num_items_test, 1,
                               name = 'doublet_embedding_b')## kucuk b  

doublet_embedding_layer = doublet_embedding(doublet_input)## buyuk W
doublet_embedding_b_layer = doublet_embedding_b(doublet_input)## kucuk b  


#### step2

res = tf.matmul(doublet_embedding_layer,tf.expand_dims(x,axis=-1))                         
res = res+doublet_embedding_b_layer
res = tf.squeeze(res,axis =2)



@tf.function
def identity_loss(y_target, y_pred):
# def identity_loss(y_target, y_pred):  
  pos, neg = tf.split(y_pred,2,1)
  positive_loss = -1*tf.math.reduce_mean(tf.math.log(tf.sigmoid(pos)))
  negative_loss = -1*tf.math.reduce_mean(tf.math.log(1- tf.sigmoid(neg)))
  total_loss = tf.math.add(positive_loss, negative_loss)
  # return tf.math.reduce_mean(y_pred)
  return total_loss
caser_model =tf.keras.Model(inputs= [
                                      user_input,
                                      seq_input,
                                      doublet_input
                                      ],
                            outputs = res,
                            # outputs = x
                            )

caser_model.compile(loss = identity_loss,optimizer = 'Adam')

caser_hist = caser_model.fit(
                            [
                              tf.constant(train_df['train_users']),
                              tf.constant(train_sequences),
                              tf.constant( np.squeeze( np.dstack( [ train_df['train_targets'].astype('int'),
                                                                   train_df['train_negatives'].astype('int') 
                                                                   ] ) ) ) ],
                y_dummy,
                epochs = 20,  
                batch_size = 512 
                )

def predict(model):
  dense_layer_weights = model.get_layer('dense_layer').weights ## k1 : shape=(120, 10) , k2 : shape=(10,)  Z
  user_embedding_weights = model.get_layer('user_embedding').weights  ## shape=(671, 10)
  doublet_embedding_b_weights = model.get_layer('doublet_embedding_b').weights
  user_embedding_weights_f = tf.reshape(user_embedding_weights, tf.shape(user_embedding_weights)[0]*
                                                              tf.shape(user_embedding_weights)[1]*
                                                              tf.shape(user_embedding_weights)[2]
                                        )
  preds = tf.math.reduce_sum(tf.concat([user_embedding_weights_f, dense_layer_weights[1]],axis = 0), axis =0)*doublet_embedding_b_weights
  x = tf.squeeze(preds)

  return x

def predict2(model, user_id):
    dense_layer_weights = caser_model.get_layer('dense_layer').weights ## k1 : shape=(120, 10) , k2 : shape=(10,)  Z
    user_embedding_weights = caser_model.get_layer('user_embedding').weights[0][user_id]  ## shape=(671, 10)
    doublet_embedding_b_weights = caser_model.get_layer('doublet_embedding_b').weights
    # doublet_embedding_b_weights = tf.squeeze(tf.squeeze(doublet_embedding_b_weights, axis = 0), axis = 1)[user_id]
    user_embedding_weights_f = tf.reshape(user_embedding_weights, 5*
                                                              1
                                                            #   tf.shape(user_embedding_weights)[2]
                                        )
    preds = tf.math.reduce_sum(tf.concat([user_embedding_weights_f, dense_layer_weights[1]],axis = 0), axis =0)*doublet_embedding_b_weights
    x = tf.squeeze(preds)
    tops = tf.math.top_k(x,k=10)[1]
    return tops

# dense_layer_weights = caser_model.get_layer('dense_layer').weights ## k1 : shape=(120, 10) , k2 : shape=(10,)  Z
# user_embedding_weights = caser_model.get_layer('user_embedding').weights[0][4]  ## shape=(671, 10)
# doublet_embedding_b_weights = caser_model.get_layer('doublet_embedding_b').weights
# doublet_embedding_b_weights = tf.squeeze(tf.squeeze(doublet_embedding_b_weights, axis = 0), axis = 1)[3]
# user_embedding_weights_f = tf.reshape(user_embedding_weights, 5*
#                                                               1
#                                                             #   tf.shape(user_embedding_weights)[2]
#                                         )
# preds = tf.math.reduce_sum(tf.concat([user_embedding_weights_f, dense_layer_weights[1]],axis = 0), axis =0)*doublet_embedding_b_weights

# caser_model.get_layer('doublet_embedding_b ').output

# for layer in caser_model.layers:
#     print(layer.name, layer)

c_model_d_emb_b = caser_model.get_layer('doublet_embedding_b').output
c_model_d_emb = caser_model.get_layer('doublet_embedding').output
c_model_x = caser_model.get_layer('tf.concat_2').output

caser_model_predict= Model(inputs = [caser_model.inputs], outputs = [c_model_d_emb_b, c_model_d_emb,c_model_x])

# caser_model_predict.summary()

bir1, iki1, uc1 = caser_model_predict([
                              tf.constant(test_df['test_users']),
                              tf.constant(test_sequences),
                              tf.constant( ( ( [ np.arange(3000)
                                                                #    train_df['train_negatives'].astype('int') 
                                           ] ) ) ) 
                                        ])
# b2,w2,x

np.arange(10)

# bir1, iki1, uc1 = caser_model_predict([
#                               tf.constant(test_df['test_users']),
#                               tf.constant(test_sequences),
#                                 tf.constant( np.squeeze( np.dstack( [ test_df['test_targets'].astype('int'),
#                                                                     test_df['test_negatives'].astype('int'),
#                                                                     test_df['test_targets'].astype('int')
#                                                                    ] ) ) )  ])

# bir, iki, uc = caser_model_predict([
#                               tf.constant(test_df['test_users'][0:2]),
#                               tf.constant(test_sequences[0:2]),
#                               tf.constant( np.squeeze( np.dstack( [ test_df['test_targets'][0:2].astype('int'),
#                                                                     test_df['test_negatives'][0:2].astype('int'),
#                                                                     test_df['test_targets'][0:2].astype('int')
#                                                                    ] ) ) ) ])



print(np.array(bir1).shape)
# print(np.array(bir).shape)

print(iki1.shape)
# print(iki.shape)

print(uc1.shape)
# print(uc.shape)

sonuc  = (tf.matmul(uc1,tf.transpose(tf.squeeze(iki1,0))))+tf.squeeze(tf.squeeze(bir1,0),1)

tops = tf.math.top_k(sonuc,k=10)[1]

train_df.head(1)



# def full_sort_predict(self, interaction):
#     item_seq = interaction[self.ITEM_SEQ]
#     user = interaction[self.USER_ID]
#     seq_output = self.forward(user, item_seq)
#     test_items_emb = self.item_embedding.weight
#     scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B, n_items]
#     return scores




