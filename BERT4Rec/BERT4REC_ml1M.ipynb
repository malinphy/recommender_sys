{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "junk4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cj09iRgUr3I4"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/ratings.dat', sep=\"::\", header=None, engine='python')\n",
        "df.columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PcczUAoLr75Q",
        "outputId": "02e57fc8-d93b-4146-a99b-892dccb3e302"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id  movie_id  rating  timestamp\n",
              "0        1      1193       5  978300760\n",
              "1        1       661       3  978302109\n",
              "2        1       914       3  978301968\n",
              "3        1      3408       4  978300275\n",
              "4        1      2355       5  978824291"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fb5fb7b-8dbc-4da0-baa9-52572fce0211\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fb5fb7b-8dbc-4da0-baa9-52572fce0211')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fb5fb7b-8dbc-4da0-baa9-52572fce0211 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fb5fb7b-8dbc-4da0-baa9-52572fce0211');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['timestamp']= pd.to_datetime(df['timestamp'], unit='s')\n",
        "def data_splitter(x):\n",
        "  return str(x).split()[0]\n",
        "\n",
        "df['timestamp'] = df['timestamp'].apply(data_splitter)\n",
        "df['timestamp'] =pd.to_datetime(df['timestamp'])\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhqyoMl0sEu_",
        "outputId": "fffc3d82-4e5b-4437-baa3-4384d9585410"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000209 entries, 0 to 1000208\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count    Dtype         \n",
            "---  ------     --------------    -----         \n",
            " 0   user_id    1000209 non-null  int64         \n",
            " 1   movie_id   1000209 non-null  int64         \n",
            " 2   rating     1000209 non-null  int64         \n",
            " 3   timestamp  1000209 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1), int64(3)\n",
            "memory usage: 30.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = df['user_id'].unique()\n",
        "movies = df['movie_id'].unique()\n",
        "num_users = len(users)\n",
        "num_movies = len(movies)\n",
        "\n",
        "print('num_users',num_users)\n",
        "print('num_movies',num_movies)\n",
        "\n",
        "movie_2enc = {i+1:j for i,j in enumerate(movies)}\n",
        "enc_2movies = {j:i+1 for i,j in enumerate(movies)}\n",
        "\n",
        "user_2enc = {i+1:j for i,j in enumerate(users)}\n",
        "enc_2users = {j:i+1 for i,j in enumerate(users)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BqOjL-us_W1",
        "outputId": "56d437bd-607f-4a19-ec47-f90df7a10101"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_users 6040\n",
            "num_movies 3706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jM7k4lX6s9pR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_movies = df['movie_id'].map(enc_2movies)\n",
        "encoded_users = df['user_id'].map(enc_2users)"
      ],
      "metadata": {
        "id": "y_lDRM1ZtDJp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame({'user_id_enc':encoded_users, 'movie_id_enc':encoded_movies, 'timestamp':df['timestamp'].copy()})\n",
        "x = df2.set_index(['user_id_enc','movie_id_enc']).sort_index()\n",
        "x = x.reset_index()\n",
        "\n",
        "x.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "3m396DOjtDZy",
        "outputId": "0a7f213b-63fd-488f-98b2-c7b9a5a245cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id_enc  movie_id_enc  timestamp\n",
              "0            1             1 2000-12-31\n",
              "1            1             2 2000-12-31\n",
              "2            1             3 2000-12-31"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ab91796-c100-444b-9db0-4aa8b3736b98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id_enc</th>\n",
              "      <th>movie_id_enc</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2000-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2000-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2000-12-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ab91796-c100-444b-9db0-4aa8b3736b98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ab91796-c100-444b-9db0-4aa8b3736b98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ab91796-c100-444b-9db0-4aa8b3736b98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.drop(columns= ['timestamp'])  "
      ],
      "metadata": {
        "id": "meuEGFKktJxi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### \n",
        "df_enc_seq = x.groupby('user_id_enc').aggregate(lambda tdf: tdf.unique().tolist())\n",
        "df_enc_seq = df_enc_seq.reset_index()\n"
      ],
      "metadata": {
        "id": "Iar-FprRtKPp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### masked language model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "3-myIQu5tMIS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "VOCAB_SIZE = num_movies + 4\n",
        "EMBED_DIM = 256\n",
        "NUM_HEAD = 8  # used in bert model\n",
        "FF_DIM = 256 +128  # used in bert model\n",
        "NUM_LAYERS = 2"
      ],
      "metadata": {
        "id": "kXjlL9TYtNr3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 32\n",
        "# LR = 0.001\n",
        "# VOCAB_SIZE = num_movies + 4\n",
        "# EMBED_DIM = 256\n",
        "# NUM_HEAD = 8  # used in bert model\n",
        "# FF_DIM = 256   # used in bert model\n",
        "# NUM_LAYERS = 2\n",
        "# epocsh = 25"
      ],
      "metadata": {
        "id": "sd2TPwA2L56i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arranged_seqs = pad_sequences(\n",
        "    df_enc_seq['movie_id_enc'], maxlen=MAX_LEN, dtype='int32', padding='pre',\n",
        "    truncating='pre', value=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "Tc21_iiOtPaS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token_id = num_movies+1\n",
        "print(mask_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ0GsYx6tQ4Z",
        "outputId": "01981a82-3215-48bf-97f0-ffc1329b20dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels"
      ],
      "metadata": {
        "id": "-a5QIKZgtSpL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_input, y_input = get_masked_input_and_labels(arranged_seqs)"
      ],
      "metadata": {
        "id": "ZYxJ1AdgtT6B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=NUM_HEAD,\n",
        "        key_dim=EMBED_DIM // NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output    \n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        VOCAB_SIZE, EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=MAX_LEN,\n",
        "        output_dim=EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(MAX_LEN, EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()"
      ],
      "metadata": {
        "id": "Y1ekbWH_tVSh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_masked_model.fit(x_input, y_input, epochs=25,\n",
        "                      # callbacks=[generator_callback]\n",
        "                      )\n",
        "# bert_masked_model.save(\"bert_mlm_imdb.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyRg5JNbtWd5",
        "outputId": "ce7ace6a-3c50-419f-899c-854e5702c4a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "189/189 [==============================] - 29s 138ms/step - loss: 3.9071\n",
            "Epoch 2/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 1.1228\n",
            "Epoch 3/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.4295\n",
            "Epoch 4/25\n",
            "189/189 [==============================] - 26s 140ms/step - loss: 0.3945\n",
            "Epoch 5/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.3747\n",
            "Epoch 6/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.3568\n",
            "Epoch 7/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.3408\n",
            "Epoch 8/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.3253\n",
            "Epoch 9/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.3106\n",
            "Epoch 10/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.2938\n",
            "Epoch 11/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.2776\n",
            "Epoch 12/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.2582\n",
            "Epoch 13/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.2372\n",
            "Epoch 14/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.2161\n",
            "Epoch 15/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.1974\n",
            "Epoch 16/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.1806\n",
            "Epoch 17/25\n",
            "189/189 [==============================] - 26s 139ms/step - loss: 0.1700\n",
            "Epoch 18/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.1504\n",
            "Epoch 19/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.1367\n",
            "Epoch 20/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.1235\n",
            "Epoch 21/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.1127\n",
            "Epoch 22/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.1019\n",
            "Epoch 23/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.0927\n",
            "Epoch 24/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.0844\n",
            "Epoch 25/25\n",
            "189/189 [==============================] - 26s 138ms/step - loss: 0.0767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8be00a2850>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = y_input[100].reshape(-1,1)\n",
        "# len(x)\n",
        "# np.where(y_input[100] == 0 )[0][-1]\n",
        "# np.random.randint(149,256)"
      ],
      "metadata": {
        "id": "Jb1xASEFDxho"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x_input, test_x_input, train_y_input, test_y_input = train_test_split(x_input, y_input, \n",
        "                                                                            train_size = 0.9, test_size = 0.1,\n",
        "                                                                            random_state = 42)\n",
        "\n",
        "print('\\n','train_x_input.shape',train_x_input.shape,'\\n', 'train_y_input.shape',train_y_input.shape,'\\n',\n",
        "      'test_x_input.shape',test_x_input.shape,'\\n','test_y_input.shape',test_y_input.shape,'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYMmh4kPGSA9",
        "outputId": "c4c27c60-d24e-4cbc-d1d1-620093fcf5dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " train_x_input.shape (5436, 256) \n",
            " train_y_input.shape (5436, 256) \n",
            " test_x_input.shape (604, 256) \n",
            " test_y_input.shape (604, 256) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(test_x_input[0])\n",
        "# print(test_y_input[0])"
      ],
      "metadata": {
        "id": "2BChOWGdHv_2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### tesk masker is designed to randomly select mask position. Since all inputs were pre-padded with 0 values,\n",
        "#### mask position will be selected from the non-zero element\n",
        "\n",
        "# def test_masker(test_input):\n",
        "#   x = test_input.copy()\n",
        "#   # x = x.reshape(-1,1)\n",
        "#   non_zero_elemnt = len(x)\n",
        "#   non_zero_pos = list(np.where(x != 0 )[0])[0]\n",
        "#   random_masking = np.random.randint(non_zero_pos,len(x))\n",
        "#   # print('nons',non_zero_pos)\n",
        "#   # print(x)\n",
        "#   x[random_masking] = mask_token_id\n",
        "\n",
        "#   # print(random_masking)\n",
        "#   return (x,3)\n",
        "\n",
        "\n",
        "# masked_test_inputs,asf = np.apply_along_axis(test_masker, 1, test_y_input)\n",
        "# (bir, iki) = np.apply_along_axis(test_masker, 1, test_y_input)"
      ],
      "metadata": {
        "id": "aJ2ZqjtpFHte"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_pos(test_input):\n",
        "  x=test_input.copy()\n",
        "  non_zero_element = len(x)\n",
        "  non_zero_pos = list(np.where(x!=0)[0])[0]\n",
        "  random_masking = np.random.randint(non_zero_pos,len(x))\n",
        "\n",
        "  return random_masking\n",
        "\n",
        "random_masking_positions = np.apply_along_axis(mask_pos, 1, test_y_input)\n",
        "\n",
        "def mask_filler(test_input):\n",
        "  x = test_input.copy()\n",
        "  for i in range(len(x)):\n",
        "    # x[random_masking_positions[i]][i] = mask_token_id\n",
        "    y = x[i]\n",
        "    y[random_masking_positions[i]] =mask_token_id\n",
        "    x[i] = y\n",
        "  return x"
      ],
      "metadata": {
        "id": "i2k78RZVpMZ7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_test_inputs = mask_filler(test_y_input)"
      ],
      "metadata": {
        "id": "GhAIXPtzUKRN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print((preds.shape))"
      ],
      "metadata": {
        "id": "IDZMkiTXePeS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GbiS5m9fVSV",
        "outputId": "5683c3fd-ea2a-400b-bf92-9777bdd2a5c1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1067, 1068, 1073, 1076, 1077, 1082, 1083, 1084, 1085, 1089, 1091,\n",
              "       1092, 1094, 1095, 1096, 1100, 1110, 1116, 1119, 1140, 1142, 1151,\n",
              "       1156, 1166, 1173, 1176, 1177, 1183, 1186, 1190, 1193, 1195, 1197,\n",
              "       1201, 1204, 1209, 1211, 1212, 1214, 1217, 1219, 1222, 1223, 1227,\n",
              "       1232, 1248, 1257, 1259, 1264, 1265, 1266, 1272, 1275, 1276, 1277,\n",
              "       1290, 1293, 1300, 1302, 1303, 1305, 1309, 1310, 1311, 1312, 1315,\n",
              "       1329, 1330, 1332, 1339, 1340, 1341, 1345, 1347, 1351, 1355, 1360,\n",
              "       1361, 1364, 1369, 1385, 1386, 1392, 1394, 1396, 1397, 1399, 1401,\n",
              "       1407, 1415, 1416, 1417, 1420, 1424, 1434, 1435, 1468, 1493, 1516,\n",
              "       1518, 1519, 1527, 1529, 1532, 1538, 1539, 1552, 1554, 1565, 1567,\n",
              "       1574, 1591, 1596, 1597, 1599, 1600, 1608, 1630, 1643, 1658, 1660,\n",
              "       1669, 1674, 1681, 1682, 1684, 1686, 1687, 1689, 1707, 1711, 1732,\n",
              "       1749, 1750, 1767, 1772, 1773, 1776, 1784, 1787, 1791, 1794, 1797,\n",
              "       1798, 1802, 1804, 1823, 1836, 1844, 1849, 1852, 1857, 1861, 1863,\n",
              "       1865, 1866, 1870, 1872, 1879, 1885, 1902, 1905, 1906, 1907, 1917,\n",
              "       1931, 1933, 1935, 1961, 1967, 1970, 1999, 2000, 2005, 2019, 2020,\n",
              "       2029, 2048, 2054, 2068, 2092, 2097, 2108, 2112, 2118, 2126, 2134,\n",
              "       2139, 2157, 2160, 2164, 2171, 2190, 2200, 2216, 2217, 2223, 2226,\n",
              "       2236, 2268, 2271, 2273, 2275, 2290, 2315, 2323, 2345, 2363, 2373,\n",
              "       2393, 2411, 2418, 2435, 2482, 2504, 2505, 2515, 2561, 2605, 2614,\n",
              "       2617, 2619, 2633, 2654, 2671, 2676, 2680, 2734, 2744, 2748, 2755,\n",
              "       2760, 2770, 2775, 2795, 2803, 2826, 2831, 2841, 2850, 2863, 2867,\n",
              "       2884, 2934, 2994, 3013, 3034, 3046, 3137, 3138, 3145, 3175, 3197,\n",
              "       3326, 3359, 3467], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_test_inputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snJQk3r8fd4d",
        "outputId": "e19ac6f1-4b58-4912-fefa-3b45e36c198e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1067, 1068, 1073, 1076, 1077, 1082, 1083, 1084, 1085, 1089, 1091,\n",
              "       1092, 1094, 1095, 1096, 1100, 1110, 1116, 1119, 1140, 1142, 1151,\n",
              "       1156, 1166, 1173, 1176, 1177, 1183, 1186, 1190, 1193, 1195, 1197,\n",
              "       1201, 1204, 1209, 1211, 1212, 1214, 1217, 1219, 1222, 1223, 1227,\n",
              "       1232, 1248, 1257, 1259, 1264, 1265, 1266, 1272, 1275, 1276, 1277,\n",
              "       1290, 1293, 1300, 1302, 1303, 1305, 1309, 1310, 1311, 1312, 1315,\n",
              "       1329, 1330, 1332, 1339, 1340, 1341, 1345, 1347, 1351, 1355, 1360,\n",
              "       1361, 1364, 1369, 1385, 1386, 1392, 1394, 1396, 1397, 1399, 1401,\n",
              "       1407, 1415, 1416, 1417, 1420, 1424, 1434, 1435, 1468, 1493, 1516,\n",
              "       1518, 1519, 1527, 1529, 1532, 1538, 1539, 1552, 1554, 1565, 1567,\n",
              "       1574, 1591, 1596, 1597, 1599, 1600, 1608, 1630, 1643, 1658, 1660,\n",
              "       1669, 1674, 1681, 1682, 1684, 1686, 1687, 1689, 1707, 1711, 1732,\n",
              "       1749, 1750, 1767, 1772, 1773, 1776, 1784, 1787, 1791, 1794, 1797,\n",
              "       1798, 1802, 1804, 1823, 1836, 1844, 1849, 1852, 1857, 1861, 1863,\n",
              "       1865, 1866, 1870, 1872, 1879, 1885, 1902, 1905, 1906, 1907, 1917,\n",
              "       1931, 1933, 1935, 1961, 1967, 1970, 1999, 2000, 2005, 2019, 2020,\n",
              "       2029, 2048, 2054, 2068, 2092, 2097, 2108, 2112, 2118, 2126, 2134,\n",
              "       2139, 2157, 2160, 2164, 2171, 2190, 2200, 2216, 2217, 2223, 2226,\n",
              "       2236, 2268, 2271, 2273, 2275, 2290, 2315, 2323, 2345, 2363, 2373,\n",
              "       2393, 2411, 2418, 2435, 2482, 2504, 2505, 2515, 2561, 2605, 2614,\n",
              "       2617, 2619, 2633, 2654, 2671, 2676, 2680, 2734, 2744, 2748, 2755,\n",
              "       3707, 2770, 2775, 2795, 2803, 2826, 2831, 2841, 2850, 2863, 2867,\n",
              "       2884, 2934, 2994, 3013, 3034, 3046, 3137, 3138, 3145, 3175, 3197,\n",
              "       3326, 3359, 3467], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = bert_masked_model.predict([masked_test_inputs[0:500]])\n",
        "mask_positions = np.where(masked_test_inputs[0:500] == mask_token_id)[1]\n",
        "# print('mask_positions',mask_positions)\n",
        "\n",
        "hit_rate_counter = 0\n",
        "for i in  range(len(preds)):\n",
        "  actual_token = test_y_input[i][mask_positions[i]] \n",
        "  # print('actual_token',actual_token)\n",
        "\n",
        "  pred_mask_pos = preds[i][mask_positions[i]]\n",
        "  top_pos = list(np.array((tf.math.top_k(pred_mask_pos, k = 10)[1])))\n",
        "  # print((top_pos))\n",
        "  if actual_token in top_pos:\n",
        "    hit_rate_counter +=1\n",
        "\n"
      ],
      "metadata": {
        "id": "VO_Mk4xIkpB_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hit_rate_counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bakQP60S2FF-",
        "outputId": "3a50d855-dda6-4e01-e38a-6d7ddffb051c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "310"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predictor():\n",
        "#   preds = bert_masked_model.predict([masked_test_inputs[0:4]])\n",
        "#   mask_positions = np.where(masked_test_inputs[0:4] == mask_token_id)[1]\n",
        "#   t = []\n",
        "#   for i in  range(len(preds)):\n",
        "#     actual_token = np.where(test_y_input[i] == mask_token_id)[0]\n",
        "  \n",
        "#     pred_mask_pos = preds[i][mask_positions[i]]\n",
        "#     top_pos = list(np.array((tf.math.top_k(pred_mask_pos, k = 6)[1])))\n",
        "#   t.append(top_pos)\n",
        "#   return(t)\n",
        "# predictor()\n",
        "# preds = bert_masked_model.predict([masked_test_inputs[0:2]])\n",
        "# mask_positions = np.where(masked_test_inputs[0:2] == mask_token_id)[1]\n",
        "# mask_positions\n",
        "# preds[:,mask_positions].shape\n",
        "# preds.shape\n",
        "# tf.math.top_k(preds[:,mask_positions], k=5)[1]"
      ],
      "metadata": {
        "id": "VBCqL3ke5zwV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oMCon-QzPrQU"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}