# -*- coding: utf-8 -*-
"""neural_col_filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17_Oz5zcuEQmb4UPO82V48mCNmvfDE8tR
"""

import pandas as pd
import numpy as np 
import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import Input,layers, Model
from tensorflow.keras.layers import Embedding,Flatten,Dense, Concatenate

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

u_data_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/u.data'
u_data_df = pd.read_csv(u_data_url, delimiter = '\t', header = None)
u_data_df.columns = ['user_id','item_id','ratings','timestamp']
u_data_df.head(1)

u_item_url ='https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/u.item'
u_item_df = pd.read_csv(u_item_url, delimiter = '|',encoding='ISO-8859-1',header = None)
# u_item_df = u_item_df.drop(columns =([3]))
genre_cols =  ['unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',
                      'Documentary', 'Drama', 'Fantasy', 'Film_Noir', 'Horror', 'Musical', 'Mystery',
                      'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western ']
                      
u_item_df.columns =  ['item_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb URL',
                      'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',
                      'Documentary', 'Drama', 'Fantasy', 'Film_Noir', 'Horror', 'Musical', 'Mystery',
                      'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western ']

u_item_df = u_item_df.drop(columns = ['video_release_date','IMDb URL'])
u_item_df = u_item_df.dropna().reset_index(drop = True)
u_item_df.head(2)

u_user_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml100K/u.user'
u_user_df = pd.read_csv(u_user_url, delimiter = '\|', header = None)
u_user_df.columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
u_user_df.head(3)

"""
Two stage recommender composed of two learning systems.
First one is named as retrieval stage. Basically,this stage
is designed for both reducing the number of recommendation and
next movie prediction. At this stage, features will be related with 
user profile such as , user_age, user_gender, user_occupation. """
"""
Second stage is ranking stage. To rank the movie recommendations for
corresponding user filtered set of movies which retrieved from first stage
(retrieval stage) will be used. At this stage, features will be realted with 
movie profile such as, movie_age, genre

In general, problem will be treated as implicit recommender system, therefore,
all rating will be converted to 1. Negative samples will be rated as 0.

"""

user_df = u_data_df.merge(u_user_df, right_on = 'user_id', left_on = 'user_id')
user_df.head(1)
user_df = user_df.merge(u_item_df ,left_on = 'item_id', right_on = 'item_id')
user_df.head(1)
user_df = user_df[['user_id', 'item_id','movie_title','age','gender','occupation','zip_code','timestamp','ratings',]]
user_df.head(1)

def time_conv(x):

  y = pd.to_datetime(x, unit='s')
  return y

user_df['timestamp'] = time_conv(user_df['timestamp'])

### I will only need time but not need the time, therefore, I will strip the time 
def data_splitter(x):
  return str(x).split()[0]

def unique_definer(df,col):
    
    unique_items = df[col].unique()
    
    return (unique_items, len(unique_items))

unique_users, num_unique_users = unique_definer(user_df,'user_id')
unique_items, num_unique_items = unique_definer(user_df,'item_id')
unique_title, num_unique_title = unique_definer(user_df,'movie_title')
unique_occupation, num_unique_occupation = unique_definer(user_df, 'occupation')

print('number of unique users :',num_unique_users)
print('number of unique items :',num_unique_items)
print('number of unique movie_title :',num_unique_title)
print('number of unique occupations :',num_unique_occupation)


def corpus_creator(unique_set):

    var_2enc = {i:j for i,j in enumerate(unique_set)}
    enc_2var = {j:i for i,j in enumerate(unique_set)}

    return var_2enc, enc_2var

user_2enc, enc_2user = corpus_creator(unique_users)
item_2enc, enc_2item = corpus_creator(unique_items)
title_2enc, enc_2title = corpus_creator(unique_title)
occ_2enc, enc_2occ = corpus_creator(unique_occupation)

user_df['user_id_enc']=user_df['user_id'].map(enc_2user)
user_df['movie_title_enc'] = user_df['movie_title'].map(enc_2title)
user_df['item_id_enc'] = user_df['item_id'].map(enc_2item)
user_df['occupation_enc'] = user_df['occupation'].map(enc_2occ)
u_item_df['item_id_enc'] = u_item_df['item_id']

user_df_enc = user_df[['user_id_enc', 'movie_title_enc','item_id_enc','occupation_enc',
                       'gender','zip_code','timestamp','ratings']].copy()

def time_sorter(df,user_column, time_column):
    x = df.set_index([user_column,time_column]).sort_index().reset_index()

    return x

user_df_enc_sorted = time_sorter(user_df_enc,'user_id_enc','timestamp')

### as discussed before we will treat the problem as explicit feedback issue. therefore
### I will converted all positive ratings into 1
user_df_enc_sorted['ratings'] = np.ones(len(user_df_enc_sorted), dtype = 'int16')

def sequencer(df,col):
    var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
    var = var.reset_index()

    return var

user_df_enc_seq = sequencer(user_df_enc_sorted,'user_id_enc')
user_df_enc_seq = user_df_enc_seq.drop(columns = ['timestamp','movie_title_enc'])
user_df_enc_seq

def train_test_maker(df): ### df should be in sequential format
    last_items = []
    for i in range(len(df)):
        last_items.append(np.array(df['item_id_enc'][i][-1]))

    mid_items = []
    for i in range(len(df)):
        mid_items.append(df['item_id_enc'][i][:-1])

    return (last_items, mid_items)

last_items, mid_items = train_test_maker(user_df_enc_seq)
user_df_enc_seq['mid_item_id_enc'] = mid_items
user_df_enc_seq['last_items'] = last_items

### this part is just cheking for the sequence length
lens = []
for i in range(len(user_df_enc_seq)):
    lens.append(len(user_df_enc_seq['item_id_enc'][i]))
np.where(np.array(lens) < 20)

max_seq_len = np.max(lens)
min_seq_len = np.min(lens)

print('maximum sequence length: ',max_seq_len)
print('minimum sequence length: ',min_seq_len)

MAX_LEN = 128
def padder(df, col, max_len):
    padded_mids = tf.keras.preprocessing.sequence.pad_sequences(
    df[col],
    # user_df_enc_seq['mid_item_id_enc'],
    maxlen=max_len,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
        )
    df['padded_mids'] = (list(padded_mids))
    return df

user_df_enc_seq = padder(user_df_enc_seq,'mid_item_id_enc', MAX_LEN)

# from typing_extensions import Concatenate
EMB_DIM = 32
user_input = Input(shape = (1,), name = 'user_input')
item_input = Input(shape =(MAX_LEN), name = 'item_input')
occ_input = Input(shape = (1,), name = 'occupation_input')


item_embedding = Embedding(num_unique_items+1, EMB_DIM , name = 'item_embeding_layer')(item_input)
flat_layer = Flatten(name = 'item_emb_flatten')(item_embedding)
concat_layer = tf.keras.layers.Concatenate(axis=1, name= 'concat_layer')([flat_layer,occ_input])
d1  = Dense(1024*2,activation = 'relu', name='dense_1')(concat_layer)
d2 = Dense(1024, activation = 'relu', name = 'dense_2')(d1)
d3 = Dense(512, activation = 'relu', name = 'dense_3')(d2)
d4 = Dense(256, activation = 'relu', name = 'dense_4')(d3)
# d5 = Dense(256, activation = 'linear', name = 'dense_5')(d4)
final = Dense(num_unique_items, activation  = 'softmax', name = 'final')(d4)

retrieval_model = Model(inputs = [user_input, item_input, occ_input], outputs = final)

retrieval_model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(name='sparse_categorical_crossentropy'),
    optimizer = 'Adam',
    metrics = ['accuracy']
                        )

padded_mids_inp = [np.array(i)  for i in (user_df_enc_seq['padded_mids'])]
occ_enc_inp = [np.array(i)  for i in (user_df_enc_seq['occupation_enc'])]
last_items_inp = [np.array(i)  for i in (user_df_enc_seq['last_items'])]

retrieval_model.fit([
                    # tf.constant(user_id_enc_train),
                    # tf.constant(padded_mids_inp_train),
                    # tf.constant(occ_enc_inp_train)

                    tf.constant(user_df_enc_seq['user_id_enc']),
                    tf.constant(padded_mids_inp),
                    tf.constant(occ_enc_inp)
                    ],
                    tf.constant(last_items_inp),
                    epochs = 5,
                    verbose = 0

                    )

user_inp_2 = Input(shape = (1,))
item_inp_2 = Input(shape = (MAX_LEN,))
occ_inp_2 = Input(shape = (1,))

# user_inp_2 = retrieval_model.get_layer('user_input')
# item_inp_2 = retrieval_model.get_layer('item_input')
# occ_inp_2 = retrieval_model.get_layer('occupation_input')

item_embedding_2 = retrieval_model.get_layer('item_embeding_layer')(item_inp_2)
flat_layer_2 = retrieval_model.get_layer('item_emb_flatten')(item_embedding_2)
concat_layer_2 = retrieval_model.get_layer('concat_layer')([flat_layer_2,occ_inp_2])
d1_2 =retrieval_model.get_layer('dense_1')(concat_layer_2)
d2_2 =retrieval_model.get_layer('dense_2')(d1_2)
d3_2 =retrieval_model.get_layer('dense_3')(d2_2)
d4_2 =retrieval_model.get_layer('dense_4')(d3_2)
# d5_2 =retrieval_model.get_layer('dense_5')(d4_2)
retrieval_model_2 =Model(inputs = [user_inp_2, item_inp_2, occ_inp_2], outputs = d4_2)

# retrieval_model_2([
#                     tf.constant(user_df_enc_seq['user_id_enc']),
#                     tf.constant(padded_mids_inp),
#                     tf.constant(occ_enc_inp)
#                     ])
out_of_soft = retrieval_model([
                    tf.constant(user_df_enc_seq['user_id_enc']),
                    tf.constant(padded_mids_inp),
                    tf.constant(occ_enc_inp)
                    ])

out_of_relu = retrieval_model_2([
                    tf.constant(user_df_enc_seq['user_id_enc']),
                    tf.constant(padded_mids_inp),
                    tf.constant(occ_enc_inp)
                    ])

top_k_number = 25
softmax_item_matrix = np.array(retrieval_model.get_layer('final').weights)[0]
relu_user_matrix = np.array(out_of_relu)
user_item_matrix = np.matmul(relu_user_matrix, (softmax_item_matrix))
top_candidates = np.array(tf.math.top_k(user_item_matrix, k=top_k_number)[1])

### AFTER THIS POINT, i WILL DEAL WITH RANKING PART

release_month = np.array(pd.DatetimeIndex(u_item_df['release_date']).month, dtype = 'int32')
release_year = np.array(pd.DatetimeIndex(u_item_df['release_date']).year, dtype = 'int32')

def seasoner(x):
    season = []
    for i in x:

        if i in [12,1,2]:
            season.append('winter')
        # season.append(1)
    
        if i in [3,4,5]:
            season.append('spring')
        # season.append(2)

        if i in [6,7,8]:
            season.append('summer')
        # season.append(3)
    
        if i in [9,10,11]:
            season.append('autumn')
        # season.append(4)

    return season

release_season = seasoner(release_month)

u_item_df['release_month'] = release_month
u_item_df['release_year'] = release_year
u_item_df['release_season'] = release_season

from datetime import datetime
today = int(datetime.today().strftime('%Y-%m-%d')[0:4])
age = today - release_year
u_item_df['age'] = age

u_item_df = u_item_df.drop(columns = genre_cols)

u_item_df['item_id_enc'] = u_item_df['item_id'].map(enc_2item)

u = []
i = []
for i in range(len(top_candidates)):
    u.append(np.full(len(top_candidates[i]),user_df_enc_seq['user_id_enc'][i]))
    
top_recs_df = pd.DataFrame({'user_id':np.concatenate(u),'tops':np.concatenate(top_candidates)})
top_recs_df.head(3)

u_item_df.head(3)

top_recs_df_2 = top_recs_df.merge(u_item_df, left_on = 'tops', right_on='item_id_enc').drop(
    columns = ['item_id']
)
top_recs_df_2.head(3)

unique_item_enc_2 = top_recs_df_2['tops'].unique()

top_recs_df_2

def sequencer(df,col):
    var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
    var = var.reset_index()

    return var

top_recs_enc_seq_df =sequencer(top_recs_df,'user_id')
print(top_recs_enc_seq_df.columns)

def train_neg_maker(num_neg, df, col):
    num_neg = num_neg
    neg3 = []
    ui= 0
    us = []
    for i in df[col]:
        pos_set = i
        neg2 = []
  
        for j in range(num_neg):
            neg1 = []
            for k in pos_set:
      
                neg_candidate = np.random.randint(1,len(unique_item_enc_2))
                while neg_candidate in pos_set:
                    neg_candidate = np.random.randint(1,len(unique_item_enc_2))
                us.append(ui)
      
                neg1.append((neg_candidate))
            neg2.append(np.array(neg1))
        ui += 1
        neg3.append(neg2)
    
    return us, neg3

us , neg3 = train_neg_maker(4,top_recs_enc_seq_df,'tops')

train_neg = pd.DataFrame({'user_id_enc':us,
              'item_id_enc_2':np.concatenate(np.concatenate(np.array(neg3))),
              'ratings':np.zeros(len(us), dtype='int32')})

train_neg

train_neg_merged = train_neg.merge(u_item_df, left_on='item_id_enc_2',
                right_on = 'item_id_enc')

train_neg_merged.head(3)

n_index = ['user_id_enc', 'item_id_enc_2', 'item_id', 'movie_title', 
       'release_date', 'item_id_enc', 'release_month', 'release_year',
       'release_season', 'age', 'ratings']

train_neg_merged = train_neg_merged[n_index]
train_neg_merged = train_neg_merged.reindex(columns = n_index)

train_neg_merged.head(3)

top_recs_df_2['ratings'] = np.ones(len(top_recs_df_2), dtype = 'int32')

top_recs_df_2['item_id_enc_j'] = top_recs_df_2['item_id_enc'].copy() 
top_recs_df_2

n_index = ['user_id_enc', 'item_id_enc_2', 'item_id', 'movie_title', 
       'release_date', 'item_id_enc', 'release_month', 'release_year',
       'release_season', 'age', 'ratings']
       
top_recs_df_2.columns

n_index_2 = ['user_id',  'item_id_enc','tops', 'movie_title',
             'release_date', 'item_id_enc_j','release_month', 'release_year',
             'release_season', 'age', 'ratings']

top_recs_df_2 = top_recs_df_2[n_index_2]
top_recs_df_2 = top_recs_df_2.reindex(columns = n_index_2)

train_neg_merged.head(3)

top_recs_df_2.head(3)

top_recs_df_2 = top_recs_df_2.rename(columns = {'user_id':'user_id_enc','tops':'item_id_enc_2','item_id_enc_j':'item_id'})

from sklearn.utils import shuffle

final_df = shuffle(pd.concat([train_neg_merged, top_recs_df_2],axis = 0))

final_df

### AFTER THIS POINT, NEURAL NETWORK FOR RANKING WILL BE BUILT AND TRAINED

top_n_corpus_size =len(final_df['item_id_enc_2'].unique())

ranking_emb_dim = 32
item_id_input_ranking = Input(shape = (1,), name='item_id_enc_2_input')
month_input_ranking = Input(shape = (1,), name = 'release_month_input')
year_input_ranking = Input(shape = (1,), name = 'release_year_input')
age_input_ranking = Input(shape = (1,), name = 'movie_age')

embedding_layer_ranking = Embedding(top_n_corpus_size*10 + 1, ranking_emb_dim)(item_id_input_ranking)
flat_layer_ranking = Flatten(name='flatten_layer_ranking')(embedding_layer_ranking)
concat_layer_ranking = tf.keras.layers.Concatenate(axis=1, name= 'concat_layer_ranking')([flat_layer_ranking,
                                                                                          month_input_ranking,
                                                                                          year_input_ranking,
                                                                                          age_input_ranking])

d1_ranking  = Dense(1024*2,activation = 'relu', name='dense_1_ranking')(concat_layer_ranking)
d2_ranking = Dense(1024, activation = 'relu', name = 'dense_2_ranking')(d1_ranking)
d3_ranking = Dense(512, activation = 'relu', name = 'dense_3_ranking')(d2_ranking)
d4_ranking = Dense(256, activation = 'relu', name = 'dense_4_ranking')(d3_ranking)
# d5 = Dense(256, activation = 'linear', name = 'dense_5')(d4)
final_ranking = Dense(num_unique_items, activation  = 'sigmoid', name = 'final_ranking')(d4_ranking)

ranking_model = Model(inputs = [item_id_input_ranking,
                                month_input_ranking,
                                year_input_ranking,
                                age_input_ranking], outputs = final_ranking)


ranking_model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(name='binary_crossentropy'),
    optimizer = 'Adam',
    metrics = ['accuracy']
                        )

ranking_model.fit([
                final_df['item_id_enc_2'],
                final_df['release_month'],
                final_df['release_year'],
                final_df['age'],   
                ],
                final_df['ratings'],
                epochs = 1
                )

