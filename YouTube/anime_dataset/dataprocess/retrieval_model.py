# -*- coding: utf-8 -*-
"""Retrieval_task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14uvUxvcP0K6Sq_A2JX8JMcnTJ-tPLJ6G
"""

import numpy as np 
import pandas as pd
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, precision_recall_fscore_support,confusion_matrix,plot_confusion_matrix,classification_report
import seaborn as sn
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import Input,layers, Model
from tensorflow.keras.layers import Embedding,Flatten,Dense, Concatenate, Dropout

### Let's define the helper functions

def time_conv(x):

    y = pd.to_datetime(x, unit='s')
    
    return y

def data_splitter(x):

    return str(x).split()[0]


def unique_definer(df,col):
    
    unique_items = df[col].unique()
    
    return (unique_items, len(unique_items))

def time_sorter(df,user_column, time_column):

    x = df.set_index([user_column,time_column]).sort_index().reset_index()

    return x

def sequencer_multi(df,col):

    # var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
    var = df.groupby(col).aggregate(lambda tdf: tdf.tolist()) 

    var = var.reset_index()

    return var

def sequencer_unique(df,col):

    var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
    # var = df.groupby(col).aggregate(lambda tdf: tdf.tolist()) 

    var = var.reset_index()

    return var

def input_label_maker(df,col,window_size, splitter): ### df should be in sequential format

    if splitter != True :
        mid_items = []
        for i in range(len(df)):
            mid_items.append(df[col][i][-window_size:])
        
        return mid_items

    if splitter == True:
        last_items = []
        for i in range(len(df)):
            last_items.append(np.array(df[col][i][-1]))

        mid_items = []
        for i in range(len(df)):
            mid_items.append(df[col][i][-window_size:-1])

        return (last_items, mid_items)

def corpus_creator(unique_set, start_index = 0):
    ### 0 padding might be need for features, hence +1 value added for dictionary index
    
    var_2enc = {i+start_index:j for i,j in enumerate(unique_set)}
    enc_2var = {j:i+start_index for i,j in enumerate(unique_set)}

    return var_2enc, enc_2var

def time_splitter(df, col):
    year = []
    month = []
    day = []

    hour = []
    min = []
    seconds = []
    for i in range(len(df)):
        date, time = str(df[col][i]).split(' ')

        x,y,z = ((date.split('-')))
        year.append(int(x))
        month.append(int(y))
        day.append(int(z))

        i,j,k = (time.split(':'))
        hour.append(int(i))
        min.append(int(j))
        seconds.append(int(k))

    return year,month, day, hour, min, seconds

def last_n_taker(df, col, N):
    dups = {k:list(v) for k,v in df.index.groupby(df[col]).items()}
    last_n_items = []
    for i in range(1,len(dups)+1):
        last_n_items.append(np.array(dups[i][0:-N], dtype = 'int32'))
    kicks = np.concatenate(last_n_items)

    return df.drop(kicks).reset_index(drop=True)

#### FOR THIS PROJECT, I WILL USE THE MOVIELENS 1M DATASET, BECAUSE 1M DATASET IS USER REACH FEATURES
movie_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/movies.dat'
movies_df = pd.read_csv(movie_url, delimiter = '::',encoding='ISO-8859-1',header = None)
movies_df.columns = ['movie_id','movie_title','genres']

ratings_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/ratings.dat'
ratings_df = pd.read_csv(ratings_url, delimiter = '::', header = None)
ratings_df.columns = ['user_id','movie_id','ratings','timestamp']

user_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/users.dat'
users_df = pd.read_csv(user_url, delimiter = '::', header = None)
users_df.columns = ['user_id','gender', 'age', 'occupation', 'zip_code']

## Since we are interested in retrieval process, we will deal with only user features, therefore
## it will be enought to use user_df and ratings_df

# print('USERS_DF INFO',users_df.info(),'\n')
# print('RATINGS_DF INFO',ratings_df.info(),'\n')
# print('MOVIES_DF INFO',movies_df.info(),'\n')

retrieval_df = ratings_df.merge(users_df, left_on = 'user_id', right_on = 'user_id')
# retrieval_df.head(3)

retrieval_df['timestamp'] = time_conv(retrieval_df['timestamp'])
retrieval_df = time_sorter(retrieval_df,'user_id', 'timestamp')
retrieval_df.head(3)

print('minimum number of movies that were interacted with user',np.min(retrieval_df.groupby(['user_id'])['movie_id'].count()))
print('maximum number of movies that were interacted with user',np.max(retrieval_df.groupby(['user_id'])['movie_id'].count()))

retrieval_df_diluted = last_n_taker(retrieval_df, 'user_id', 20)

unique_users, num_unique_users = unique_definer(retrieval_df_diluted,'user_id')
unique_movies, num_unique_movies = unique_definer(retrieval_df_diluted,'movie_id')
# unique_title, num_unique_title = unique_definer(retrieval_df_diluted,'movie_title')
unique_occupations, num_unique_occupations = unique_definer(retrieval_df_diluted, 'occupation')
unique_zips, num_unique_zips = unique_definer(retrieval_df_diluted, 'zip_code')

user_2enc, enc_2user = corpus_creator(unique_users, start_index = 0)
item_2enc, enc_2movie = corpus_creator(unique_movies, start_index = 0)
# title_2enc, enc_2title = corpus_creator(unique_title, start_index = 0)
occ_2enc, enc_2occ = corpus_creator(unique_occupations, start_index = 0)
zip_2enc, enc_2zip = corpus_creator(unique_zips, start_index = 0)

LE_gender = LabelEncoder()
LE_age = LabelEncoder()
LE_ge = LabelEncoder()
retrieval_df_enc = pd.DataFrame({
                                'user_id_enc' : retrieval_df_diluted['user_id'].map(enc_2user) ,
                                'movie_id_enc' : retrieval_df_diluted['movie_id'].map(enc_2movie) ,
                                'occupation_enc' : retrieval_df_diluted['occupation'].map(enc_2occ) ,
                                'gender_enc' : LE_gender.fit_transform(retrieval_df_diluted['gender']) ,
                                'age_enc' : LE_age.fit_transform(retrieval_df_diluted['age']) ,
                                'zip_code_enc' : retrieval_df_diluted['zip_code'].map(enc_2zip) ,
                                'timestamp' : retrieval_df_diluted['timestamp'].copy()
                                })
# retrieval_df_enc.head(3)

seq_retrieval_df_enc = sequencer_multi(retrieval_df_enc[['user_id_enc','movie_id_enc']],'user_id_enc')
seq_retrieval_df_enc_2 = sequencer_unique(retrieval_df_enc,'user_id_enc')
seq_retrieval_df_enc['occupation_enc'] = seq_retrieval_df_enc_2['occupation_enc']
seq_retrieval_df_enc['gender_enc'] = seq_retrieval_df_enc_2['gender_enc']
seq_retrieval_df_enc['age_enc'] = seq_retrieval_df_enc_2['age_enc']
seq_retrieval_df_enc['zip_code_enc'] = seq_retrieval_df_enc_2['zip_code_enc']

del seq_retrieval_df_enc_2

seq_retrieval_df_enc

last_movie_enc, input_movie_enc = input_label_maker(seq_retrieval_df_enc,'movie_id_enc',20, splitter = True)
seq_retrieval_df_enc['last_movie_enc'] = last_movie_enc
seq_retrieval_df_enc['input_movie_enc'] = input_movie_enc

train_df, test_df = train_test_split(seq_retrieval_df_enc, train_size = 0.8, test_size = 0.2, random_state = 42)

input_movie_enc_train = [np.array(i)  for i in (train_df['input_movie_enc'])]
last_movie_enc_train = [np.array(i)  for i in (train_df['last_movie_enc'])]
occupation_enc_train = [np.array(i)  for i in (train_df['occupation_enc'])]
gender_enc_train = [np.array(i)  for i in (train_df['gender_enc'])]
age_enc_train = [np.array(i)  for i in (train_df['age_enc'])]
zip_code_enc_train = [np.array(i)  for i in (train_df['zip_code_enc'])]




input_movie_enc_test = [np.array(i)  for i in (test_df['input_movie_enc'])]
last_movie_enc_test = [np.array(i)  for i in (test_df['last_movie_enc'])]
occupation_enc_test = [np.array(i)  for i in (test_df['occupation_enc'])]
gender_enc_test = [np.array(i)  for i in (test_df['gender_enc'])]
age_enc_test = [np.array(i)  for i in (test_df['age_enc'])]
zip_code_enc_test = [np.array(i)  for i in (test_df['zip_code_enc'])]
train_df.head(1)

movie_inp = Input(shape =(19,), name='item_input')
occ_inp = Input(shape =(1,), name='occ_input')
gen_inp = Input(shape =(1,), name='gender_input')
age_inp = Input(shape =(1,), name='age_input')
zip_code_inp = Input(shape =(1,), name='zip_code_input')

emb_layer = Embedding(num_unique_movies+1, 256)(movie_inp)
# gru_layer = GRU(512, activation= 'tanh')(emb_layer)
flat_layer = Flatten(name = 'flatten_layer')(emb_layer)
cancat_layer = tf.keras.layers.Concatenate(axis=1)([flat_layer,occ_inp,gen_inp,age_inp,zip_code_inp])

d1 = Dense(1024, activation = 'relu' , name = 'd1_layer')(flat_layer)
d2 = Dense(512, activation = 'relu', name = 'd2_layer')(d1)
d3 = Dense(256, activation = 'relu', name = 'd3_layer')(d2)
final = Dense(num_unique_movies, activation = 'softmax', name = 'softmax_layer')(d3)

ret_model = Model(inputs = [movie_inp,occ_inp,gen_inp,age_inp,zip_code_inp], outputs = final)

ret_model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer = 'Adam',
    metrics = ['accuracy'],
)

ret_model.fit(
    [tf.constant(input_movie_enc_train),
     tf.constant(occupation_enc_train),
     tf.constant(gender_enc_train),
     tf.constant(age_enc_train),
     tf.constant(zip_code_enc_train)],
    tf.constant(last_movie_enc_train),
     epochs = 3,
    #  batch_size = 1000
     )

item_embeddings = ((ret_model.get_layer("softmax_layer").weights)[0])
print('item_embeddings_shape',(item_embeddings).shape)

user_emb_model_test = Model(inputs = ret_model.inputs , outputs = ret_model.get_layer('d3_layer').output )
user_embeddings_test = user_emb_model_test([tf.constant(input_movie_enc_test),
                                            tf.constant(occupation_enc_test),
                                            tf.constant(gender_enc_test),
                                            tf.constant(age_enc_test),
                                            tf.constant(zip_code_enc_test)])
print('user_embeddings_test_shape',user_embeddings_test.shape)
user_item_test_matrix = tf.matmul(user_embeddings_test, item_embeddings)
user_item_test_matrix.shape

K = 10
top_recommended_items_enc = tf.math.top_k(user_item_test_matrix, k=K)[1]

import numpy as np
def apk(actual, predicted, k):
    """
    Computes the average precision at k.
    This function computes the average prescision at k between two lists of
    items.
    Parameters
    ----------
    actual : list
             A list of elements that are to be predicted (order doesn't matter)
    predicted : list
                A list of predicted elements (order does matter)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The average precision at k over the input lists
    """
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    if not actual:
        return 0.0

    return score / min(len(actual), k)

def mapk(actual, predicted, k):
    """
    Computes the mean average precision at k.
    This function computes the mean average prescision at k between two lists
    of lists of items.
    Parameters
    ----------
    actual : list
             A list of lists of elements that are to be predicted 
             (order doesn't matter in the lists)
    predicted : list
                A list of lists of predicted elements
                (order matters in the lists)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

print(mapk(np.array(last_movie_enc_test).reshape(-1,1), top_recommended_items_enc, k=K))


