# -*- coding: utf-8 -*-
"""junk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U2jVUOc2iFZ0luSRFL3az8QQXNOTK43t
"""

import pandas as pd
import numpy as np 
import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras import Input,layers, Model
from tensorflow.keras.layers import Embedding,Flatten,Dense, Concatenate, Dropout

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from sklearn.metrics import precision_score, precision_recall_fscore_support,confusion_matrix,plot_confusion_matrix,classification_report
import seaborn as sn
import matplotlib.pyplot as plt

u_data_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/ratings.dat'
u_data_df = pd.read_csv(u_data_url, delimiter = '::', header = None)
u_data_df.columns = ['user_id','item_id','ratings','timestamp']
u_data_df.head(1)

u_item_url ='https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/movies.dat'
u_item_df = pd.read_csv(u_item_url, delimiter = '::',encoding='ISO-8859-1',header = None)
# u_item_df = u_item_df.drop(columns =([3]))
# genre_cols =  ['unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',
#                       'Documentary', 'Drama', 'Fantasy', 'Film_Noir', 'Horror', 'Musical', 'Mystery',
#                       'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western ']
                      
# u_item_df.columns =  ['item_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb URL',
#                       'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',
#                       'Documentary', 'Drama', 'Fantasy', 'Film_Noir', 'Horror', 'Musical', 'Mystery',
#                       'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western ']

u_item_df.columns = ['item_id','movie_title','Genres']

# u_item_df = u_item_df.drop(columns = ['video_release_date'])
u_item_df.head(2)

u_user_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/users.dat'
u_user_df = pd.read_csv(u_user_url, delimiter = '::', header = None)
u_user_df.columns = ['user_id','gender', 'age', 'occupation', 'zip_code']
u_user_df.head(3)

user_df = u_data_df.merge(u_user_df, right_on = 'user_id', left_on = 'user_id')
user_df.head(1)
user_df = user_df.merge(u_item_df ,left_on = 'item_id', right_on = 'item_id')
user_df.head(1)
user_df = user_df[['user_id', 'item_id','movie_title','age','gender','occupation','zip_code','timestamp','ratings',]]
user_df.head(1)

def time_conv(x):

    y = pd.to_datetime(x, unit='s')
    
    return y

def data_splitter(x):

    return str(x).split()[0]


def unique_definer(df,col):
    
    unique_items = df[col].unique()
    
    return (unique_items, len(unique_items))

user_df['timestamp'] = time_conv(user_df['timestamp'])

unique_users, num_unique_users = unique_definer(user_df,'user_id')
unique_items, num_unique_items = unique_definer(user_df,'item_id')
unique_title, num_unique_title = unique_definer(user_df,'movie_title')
unique_occupation, num_unique_occupation = unique_definer(user_df, 'occupation')

def corpus_creator(unique_set):
    ### 0 padding might be need for features, hence +1 value added for dictionary index
    var_2enc = {i+1:j for i,j in enumerate(unique_set)}
    enc_2var = {j:i+1 for i,j in enumerate(unique_set)}

    return var_2enc, enc_2var

user_2enc, enc_2user = corpus_creator(unique_users)
item_2enc, enc_2item = corpus_creator(unique_items)
title_2enc, enc_2title = corpus_creator(unique_title)
occ_2enc, enc_2occ = corpus_creator(unique_occupation)

user_df['user_id_enc']=user_df['user_id'].map(enc_2user)
user_df['movie_title_enc'] = user_df['movie_title'].map(enc_2title)
user_df['item_id_enc'] = user_df['item_id'].map(enc_2item)
user_df['occupation_enc'] = user_df['occupation'].map(enc_2occ)

user_df_enc = user_df[['user_id_enc', 'movie_title_enc','item_id_enc','occupation_enc','gender','zip_code','timestamp','ratings']].copy()

def time_sorter(df,user_column, time_column):

    x = df.set_index([user_column,time_column]).sort_index().reset_index()

    return x

def sequencer(df,col):

    var = df.groupby(col).aggregate(lambda tdf: tdf.unique().tolist()) 
    var = var.reset_index()

    return var

user_df_enc_sorted = time_sorter(user_df_enc,'user_id_enc','timestamp')
user_df_enc_sorted['ratings'] = np.ones(len(user_df_enc_sorted), dtype = 'int16')

user_df_enc_seq = sequencer(user_df_enc_sorted,'user_id_enc')
user_df_enc_seq = user_df_enc_seq.drop(columns = [
                                                'timestamp',
                                                # 'movie_title_enc'
                                                ])
user_df_enc_seq.head(3)

def input_label_maker(df): ### df should be in sequential format

    last_items = []
    for i in range(len(df)):
        last_items.append(np.array(df['item_id_enc'][i][-1]))

    mid_items = []
    for i in range(len(df)):
        mid_items.append(df['item_id_enc'][i][:-1])

    return (last_items, mid_items)

last_items, mid_items = input_label_maker(user_df_enc_seq)

user_df_enc_seq['mid_item_id_enc'] = mid_items
user_df_enc_seq['last_items'] = last_items

user_df_enc_seq.head(3)

seq_lens = []
for i in range(len(user_df_enc_seq)):
    seq_lens.append(len(user_df_enc_seq['item_id_enc'][i]))
np.where(np.array(seq_lens) < 20)

max_seq_len = np.max(seq_lens)
min_seq_len = np.min(seq_lens)

print('maximum sequence length: ',max_seq_len)
print('minimum sequence length: ',min_seq_len)

plt.style.use('ggplot')
plt.hist(seq_lens,bins = np.arange(0,400,8))

MAX_LEN = 64+16
def padder(df, col, max_len):
    padded_mids = tf.keras.preprocessing.sequence.pad_sequences(
    df[col],
    # user_df_enc_seq['mid_item_id_enc'],
    maxlen=max_len,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
        )
    df['padded_mids'] = (list(padded_mids))
    return df

user_df_enc_seq = padder(user_df_enc_seq,'mid_item_id_enc', MAX_LEN)
user_df_enc_seq.head(3)
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(user_df_enc_seq, train_size = 0.8, test_size = 0.2, random_state = 42)

padded_mids_inp_train = [np.array(i)  for i in (train_df['padded_mids'])]
occ_enc_inp_train = [np.array(i)  for i in (train_df['occupation_enc'])]
last_items_inp_train = [np.array(i)  for i in (train_df['last_items'])]

padded_mids_inp_test = [np.array(i)  for i in (test_df['padded_mids'])]
occ_enc_inp_test = [np.array(i)  for i in (test_df['occupation_enc'])]
last_items_inp_test = [np.array(i)  for i in (test_df['last_items'])]

# padded_mids_inp_train = [np.array(i)  for i in (train_df['mid_item_id_enc'])]
# occ_enc_inp_train = [np.array(i)  for i in (train_df['occupation_enc'])]
# last_items_inp_train = [np.array(i)  for i in (train_df['last_items'])]

# padded_mids_inp_test = [np.array(i)  for i in (test_df['mid_item_id_enc'])]
# occ_enc_inp_test = [np.array(i)  for i in (test_df['occupation_enc'])]
# last_items_inp_test = [np.array(i)  for i in (test_df['last_items'])]

# from typing_extensions import Concatenate
EMB_DIM = 64
user_input = Input(shape = (1,), name = 'user_input')
item_input = Input(shape =(MAX_LEN), name = 'item_input')
occ_input = Input(shape = (1,), name = 'occupation_input')


item_embedding = Embedding(num_unique_items+1, EMB_DIM , name = 'item_embeding_layer')(item_input)
flat_layer = Flatten(name = 'item_emb_flatten')(item_embedding)
concat_layer = tf.keras.layers.Concatenate(axis=1, name= 'concat_layer')([flat_layer,occ_input])
d1  = Dense(1024*2,activation = 'relu', name='dense_1')(concat_layer)
d1 = Dropout(0.6)(d1)
d2 = Dense(1024, activation = 'relu', name = 'dense_2')(d1)
d2 = Dropout(0.6)(d2)
d3 = Dense(512, activation = 'relu', name = 'dense_3')(d2)
d3 = Dropout(0.6)(d3)
d4 = Dense(256, activation = 'relu', name = 'dense_4')(d3)
# d5 = Dense(256, activation = 'linear', name = 'dense_5')(d4)
# d4 = Dropout(0.8)(d4)
final = Dense(num_unique_items, activation  = 'softmax', name = 'final')(d4)

retrieval_model = Model(inputs = [user_input, item_input, occ_input], outputs = final)

retrieval_model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(name='sparse_categorical_crossentropy'),
    optimizer = 'Adam',
    metrics = ['accuracy']
)

retrieval_hist = retrieval_model.fit([
                    # tf.constant(user_id_enc_train),
                    # tf.constant(padded_mids_inp_train),
                    # tf.constant(occ_enc_inp_train)

                    tf.constant(train_df['user_id_enc']),
                    tf.constant(padded_mids_inp_train),
                    tf.constant(occ_enc_inp_train)
                    ],
                    tf.constant(last_items_inp_train),
                    epochs = 30,
                    verbose = 0,
                    validation_split = 0.2

                    )

retrieval_hist.history.keys()

plt.plot(retrieval_hist.history['loss'])
plt.plot(retrieval_hist.history['val_loss'])

plt.plot(retrieval_hist.history['accuracy'])
plt.plot(retrieval_hist.history['val_accuracy'])

pred = retrieval_model.predict([ 
                                tf.constant(test_df['user_id_enc']),
                                tf.constant(padded_mids_inp_test),
                                tf.constant(occ_enc_inp_test)])
pred.shape
K = 10
top_k_pred = np.array(tf.math.top_k(pred, k=K)[1])

def apk(actual, predicted, k=10):
    """
    Computes the average precision at k.
    This function computes the average prescision at k between two lists of
    items.
    Parameters
    ----------
    actual : list
             A list of elements that are to be predicted (order doesn't matter)
    predicted : list
                A list of predicted elements (order does matter)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The average precision at k over the input lists
    """
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    if not actual:
        return 0.0

    return score / min(len(actual), k)

def mapk(actual, predicted, k):
    """
    Computes the mean average precision at k.
    This function computes the mean average prescision at k between two lists
    of lists of items.
    Parameters
    ----------
    actual : list
             A list of lists of elements that are to be predicted 
             (order doesn't matter in the lists)
    predicted : list
                A list of lists of predicted elements
                (order matters in the lists)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

mapk(np.array(last_items_inp_test).reshape(-1,1),top_k_pred, k =K)

