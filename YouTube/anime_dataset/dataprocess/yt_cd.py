# -*- coding: utf-8 -*-
"""YT_cd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YEAoVdc4--YwFN3pj9KFAIUe1T2BZbNi
"""

from google.colab import drive 
drive.mount('/content/drive')
import numpy as np 
import pandas as pd 
import os 
import sys
import matplotlib.pyplot as plt 
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers 
from tensorflow.keras.layers import Dense, Embedding, Concatenate,Flatten,Lambda,BatchNormalization 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

class data_prep:
  def __init__(self,dir):
    self.dir = dir
  
  def prep(self):
    animelist_cleaned_cols = ['username','anime_id','my_last_updated'] ##since data set is large, 
                                                                       ##I will not add some non-related columns
    df = pd.read_csv(self.dir,
                     usecols = animelist_cleaned_cols,
                    #  nrows = 100
                     )
    

    date_part_1 = []   #### "last_updated" column contains both date and hours
                       ### however, I will not make sorting according to hours,
                       ### therfore time part will be stipped
    for i in df['my_last_updated']:
      x = (str(i).split(' ')[0])
      date_part_1.append(x)

    df['my_last_updated'] =date_part_1
    #### Above column will convert the string data into a date data
    earlier_index = np.where(df['my_last_updated'] < '2000-00-00')
    df = df.drop(earlier_index[0]).reset_index(drop= True)
    df['my_last_updated'] = pd.to_datetime(df['my_last_updated'])
    


    #### unique users and corresponding anime data will be grouped and will be sorted as 
    #### function of time as ascendin order 
    x = df.set_index(['username','my_last_updated']).sort_index()
    #### grouped date will be converted into pandas dataframe with suitable indexes
    x = x.reset_index()
    x = x.drop(columns= ['my_last_updated'])  
    #### all animes were sorted as function of time and transposed according to username 
    df = x.groupby('username').aggregate(lambda tdf: tdf.unique().tolist())
    df = df.reset_index()
    
    ### users who watched less than min_len animes , has been dropped
    min_len = 5
    shorts = []
    for i,j in enumerate(df['anime_id']):
      if len(j) < min_len:
        shorts.append(i)

    df = df.drop(shorts).reset_index(drop=True)
    last_watches = []
    previous_watches = []
    for i in df['anime_id']:
      last_watches.append(i[-1])
      previous_watches.append(i[:-1])
    df['previous_watches'] = previous_watches
    df['last_watches'] = last_watches
    df = df.drop(columns = ['anime_id'])

    np_prev_w = []
    for i in  df['previous_watches'] :
      var1 = []
      for j in i:
        var1.append(int(j))
      np_prev_w.append(np.array(var1))

    df['previous_watches'] = np_prev_w
    return df 
dir_url = 'drive/MyDrive/Colab Notebooks/datasets/anime/animelists_cleaned.csv'
animelists = data_prep(dir_url).prep()

animelists['previous_watches'][0]

max_watch_len = 15
short_sequence = [] 
last_max = []
for i in range(len(animelists['previous_watches'])):
  if len(animelists['previous_watches'][i]) < max_watch_len:
    short_sequence.append(i)
  last_max.append(animelists['previous_watches'][i][-max_watch_len:])

animelists['previous_watches'] = last_max
animelists =animelists.drop(short_sequence).reset_index(drop = True)

# animelists = animelists.dropna().reset_index(drop=True)
# animelists = animelists.reset_index(drop=True)







users =pd.read_csv('drive/MyDrive/Colab Notebooks/datasets/anime/users_cleaned_dpep.csv')
users = users.drop(columns = ['Unnamed: 0','index'])

### To map all anime_id, I will stack previous_watches and last_watches and extract the unique id
x = np.hstack(animelists['previous_watches'])
x2 = np.hstack((x , animelists['last_watches']))
unique_anime = np.unique(x2)
anime_corpus = len(unique_anime)

# dictionaries for the anime_id and encoded values(mapped values)
anime_2enc = { i:j for i,j in enumerate(unique_anime)}
enc_2anime = { j:i for i,j in enumerate(unique_anime)}

# last_watches_encoded = (animelists['last_watches'].map(enc_2anime))
last_watches_encoded = []
for i in animelists['last_watches']:
  last_watches_encoded.append(enc_2anime[i])

animelists

previous_watches_encoded = []
for i in animelists['previous_watches']:
  var1 = []
  for j in i:
    var1.append(enc_2anime[j])
  previous_watches_encoded.append(var1)

animelists['previous_watches_encoded'] = previous_watches_encoded
animelists['last_watches_encoded'] = (last_watches_encoded)

animelists

animelists['previous_watches'][0]



users.head()

final_df = animelists.merge(users, left_on = 'username', right_on='username')
LB_gender = LabelEncoder()
gender_encoded = LB_gender.fit_transform(final_df['gender'])
final_df['gender_encoded'] = gender_encoded

LB_country = LabelEncoder()
country_encoded = LB_country.fit_transform(final_df['countries'])
final_df['country_encoded'] = country_encoded

final_df.head()



#### after this point, inputs will be defined :
#### previous_watches_encoded, user_watching 	user_completed 	user_onhold 	user_dropped 	user_plantowatch, gender, country,user_age will be inputs,
#### previous_watches_encoded, will be embedded, other non-integer values, such as country will be labeled
#### last_watches_encoded is y_logits of the neural net

max(final_df['last_watches_encoded'])

emb_dim = 16
previous_watches_input = tf.keras.Input(shape = (15,), name = 'previous_watches_encoded_input')
user_watching_input = tf.keras.Input(shape = (1,), name = 'user_watching_input')
user_completed_input = tf.keras.Input(shape = (1,), name = 'user_completed_input')
user_onhold_input = tf.keras.Input(shape = (1,), name= 'user_onhold_input')
user_gender_input = tf.keras.Input(shape = (1,), name = 'user_gender_encoded_input')
user_country_input = tf.keras.Input(shape = (1,), name = 'user_country_encoded_input')

emb_layer = Embedding(anime_corpus, emb_dim, name = 'watch_history_emb_layer')(previous_watches_input)
flat_layer = Flatten(name = 'Flatten_layer')(emb_layer)
concat_layer = Concatenate(axis = 1, name= 'Concatenation_layer')([flat_layer,user_watching_input,
                                                                   user_completed_input,user_onhold_input,user_gender_input,user_country_input ])
d1_layer = Dense(256, activation ='relu', name= 'dense_layer_1')(concat_layer)
d2_layer = Dense(128, activation ='relu', name= 'dense_layer_2')(d1_layer)
d3_layer = Dense(64, activation = 'relu', name= 'dense_layer_3')(d2_layer)
d4_layer = Dense(max(final_df['last_watches_encoded'])+1, activation='softmax', name= 'final_layer')(d3_layer)


candidate_generation_model = tf.keras.Model(inputs= [previous_watches_input,
                        user_watching_input,
                        user_completed_input,
                        user_onhold_input,
                        user_gender_input,
                        user_country_input
                        ],
               outputs = d4_layer)

# tf.keras.utils.plot_model(
#     candidate_generation_model,  show_shapes=True, show_dtype=True,
#     show_layer_names=True,
#      show_layer_activations=True
# )

last_watches_encoded_  =  np.array([int(i) for i in  final_df['last_watches_encoded']], dtype= 'int32')
final_df.head(1)
final_df['user_watching']  =final_df['user_watching'].astype('int32')
final_df['user_completed'] = (final_df['user_completed']).astype('int32')
final_df['user_onhold'] = final_df['user_onhold'].astype('int32')
final_df['gender_encoded'] = final_df['gender_encoded'].astype('int32')
final_df['country_encoded'] = final_df['country_encoded'].astype('int32')
final_df['last_watches_encoded'] = last_watches_encoded_
array_inp =[np.array(i)  for i in (final_df['previous_watches_encoded'])]



candidate_generation_model.compile(
    optimizer = 'Adam',
    loss = 'SparseCategoricalCrossentropy',
    metrics = ['accuracy']
)

candidate_generation_history = candidate_generation_model.fit([
                                pad_sequences(array_inp),
                                (final_df['user_watching']),
                                (final_df['user_completed']),
                                (final_df['user_onhold']),
                                (final_df['gender_encoded']),
                                (final_df['country_encoded'])
                                ],
                               (final_df['last_watches_encoded']),
                               epochs= 200
                               )

[print(i.shape, i.dtype) for i in candidate_generation_model.inputs]
print('   ')
[print(o.shape, o.dtype) for o in candidate_generation_model.outputs]





